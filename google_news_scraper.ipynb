{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/derek881107/Real-Time-Disaster-Detection-System/blob/main/google_news_scraper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JvEXSTgP_zi2"
      },
      "outputs": [],
      "source": [
        "pip install openai>=1.0.0 pygooglenews beautifulsoup4 pandas plotly requests lxml openpyxl numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qf8QiIA0vLLZ",
        "outputId": "a8870739-20b6-4131-cdbb-8f4c8922378d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using OpenAI v1.0+ API\n",
            "✅ Using PyGoogleNews API\n",
            "\n",
            "================================================================================\n",
            "ENHANCED DISASTER NEWS ANALYSIS SYSTEM v3.3 - PYGOOGLENEWS WITH LOCALIZATION\n",
            "================================================================================\n",
            "Features: PyGoogleNews API • GPT Country/Language Detection • Localized Search • Client Address Risk Assessment\n",
            "NEW: PyGoogleNews replaces RSS feeds with GPT-powered localization and client address-based risk scoring\n",
            "Capabilities: Localized news search • GPT country analysis • Client address risk scoring • Comprehensive Excel export\n",
            "Status: Production Ready • PyGoogleNews Active • GPT Localization Active • Client Address Risk Scoring Active\n",
            "================================================================================\n",
            "Quick Start: run_enhanced_disaster_analysis()\n",
            "Demo Mode: quick_demo_analysis()\n",
            "Batch Mode: batch_analysis(api_key, excel_path, start_row, num_rows)\n",
            "Health Check: system_health_check()\n",
            "================================================================================\n",
            "ENHANCED DISASTER NEWS ANALYSIS SYSTEM v3.3\n",
            "================================================================================\n",
            "Complete system with PyGoogleNews, GPT localization, and client address risk scoring\n",
            "PyGoogleNews API with GPT country/language detection and localized disaster terminology\n",
            "Automatic From_Date/To_Date extraction from Excel for targeted news search\n",
            "Client address-based risk assessment replacing country codes\n",
            "================================================================================\n",
            "Unknown option: -f. Use --help for available options.\n"
          ]
        }
      ],
      "source": [
        "# COMPLETE ENHANCED DISASTER NEWS ANALYSIS SYSTEM v3.3\n",
        "# Integration of PyGoogleNews API with Localized Search and GPT Language Detection\n",
        "# Enhanced Disaster Analysis System with PyGoogleNews and Client Country Localization\n",
        "# ==========================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import json\n",
        "import requests\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "from bs4 import BeautifulSoup\n",
        "import html\n",
        "import re\n",
        "from dataclasses import dataclass, field\n",
        "import traceback\n",
        "import subprocess\n",
        "import logging\n",
        "import urllib.parse\n",
        "import email.utils\n",
        "import calendar\n",
        "\n",
        "# Install required packages\n",
        "def install_packages():\n",
        "    \"\"\"Install all required packages for the disaster analysis system\"\"\"\n",
        "    packages = [\n",
        "        \"openai>=1.0.0\", \"pygooglenews\", \"beautifulsoup4\", \"pandas\",\n",
        "        \"plotly\", \"requests\", \"lxml\", \"openpyxl\", \"numpy\"\n",
        "    ]\n",
        "\n",
        "    for package in packages:\n",
        "        try:\n",
        "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"--quiet\"])\n",
        "            print(f\"✅ {package} installed successfully\")\n",
        "        except:\n",
        "            print(f\"⚠️ {package} installation failed - using fallback\")\n",
        "\n",
        "# Handle OpenAI import\n",
        "try:\n",
        "    from openai import OpenAI\n",
        "    OPENAI_V1 = True\n",
        "    print(\"✅ Using OpenAI v1.0+ API\")\n",
        "except ImportError:\n",
        "    import openai\n",
        "    OPENAI_V1 = False\n",
        "    print(\"✅ Using OpenAI legacy API\")\n",
        "\n",
        "# Handle PyGoogleNews import\n",
        "try:\n",
        "    from pygooglenews import GoogleNews\n",
        "    print(\"✅ Using PyGoogleNews API\")\n",
        "except ImportError:\n",
        "    print(\"❌ PyGoogleNews not available. Please install: pip install pygooglenews\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# ==========================================\n",
        "# COUNTRY-LANGUAGE MAPPING AND GPT LOCALIZATION\n",
        "# ==========================================\n",
        "\n",
        "# Default country-language mappings for fallback\n",
        "DEFAULT_COUNTRY_LANG_MAP = {\n",
        "    'US': {'lang': 'en', 'country': 'US'},\n",
        "    'MX': {'lang': 'es', 'country': 'MX'},\n",
        "    'CN': {'lang': 'zh', 'country': 'CN'},\n",
        "    'HK': {'lang': 'zh', 'country': 'HK'},\n",
        "    'SG': {'lang': 'en', 'country': 'SG'},\n",
        "    'IN': {'lang': 'en', 'country': 'IN'},\n",
        "    'MY': {'lang': 'en', 'country': 'MY'},\n",
        "    'NL': {'lang': 'nl', 'country': 'NL'},\n",
        "    'RO': {'lang': 'ro', 'country': 'RO'},\n",
        "    'HU': {'lang': 'hu', 'country': 'HU'},\n",
        "    'JP': {'lang': 'ja', 'country': 'JP'},\n",
        "    'KR': {'lang': 'ko', 'country': 'KR'},\n",
        "    'TW': {'lang': 'zh', 'country': 'TW'},\n",
        "    'TH': {'lang': 'th', 'country': 'TH'},\n",
        "    'VN': {'lang': 'vi', 'country': 'VN'},\n",
        "    'PH': {'lang': 'en', 'country': 'PH'},\n",
        "    'AU': {'lang': 'en', 'country': 'AU'},\n",
        "    'CA': {'lang': 'en', 'country': 'CA'},\n",
        "    'GB': {'lang': 'en', 'country': 'GB'},\n",
        "    'DE': {'lang': 'de', 'country': 'DE'},\n",
        "    'FR': {'lang': 'fr', 'country': 'FR'},\n",
        "    'IT': {'lang': 'it', 'country': 'IT'},\n",
        "    'ES': {'lang': 'es', 'country': 'ES'},\n",
        "    'BR': {'lang': 'pt', 'country': 'BR'},\n",
        "    'AR': {'lang': 'es', 'country': 'AR'},\n",
        "    'CL': {'lang': 'es', 'country': 'CL'},\n",
        "    'CO': {'lang': 'es', 'country': 'CO'},\n",
        "    'PE': {'lang': 'es', 'country': 'PE'},\n",
        "}\n",
        "\n",
        "class CountryLanguageAnalyzer:\n",
        "    \"\"\"GPT-powered country language analyzer for localized disaster news search\"\"\"\n",
        "\n",
        "    def __init__(self, openai_client, use_legacy_api=False):\n",
        "        self.openai_client = openai_client\n",
        "        self.use_legacy_api = use_legacy_api\n",
        "        self.country_lang_cache = {}  # Cache for GPT responses\n",
        "\n",
        "    def analyze_client_country_language(self, client_country: str, client_address: str = \"\") -> Dict[str, str]:\n",
        "        \"\"\"Use GPT to analyze client country and determine appropriate language and country codes\"\"\"\n",
        "\n",
        "        if not client_country or client_country.strip() in ['', 'N/A', 'Unknown']:\n",
        "            return {'lang': 'en', 'country': 'US'}  # Default fallback\n",
        "\n",
        "        # Check cache first\n",
        "        cache_key = f\"{client_country}_{client_address}\".lower().strip()\n",
        "        if cache_key in self.country_lang_cache:\n",
        "            return self.country_lang_cache[cache_key]\n",
        "\n",
        "        # Check if it's a known country code\n",
        "        client_upper = client_country.upper().strip()\n",
        "        if client_upper in DEFAULT_COUNTRY_LANG_MAP:\n",
        "            result = DEFAULT_COUNTRY_LANG_MAP[client_upper]\n",
        "            self.country_lang_cache[cache_key] = result\n",
        "            return result\n",
        "\n",
        "        try:\n",
        "            prompt = f\"\"\"\n",
        "You are a geographical and linguistic expert. Analyze the client location information and determine the appropriate language and country codes for Google News search.\n",
        "\n",
        "Client Information:\n",
        "- Client Country: {client_country}\n",
        "- Client Address: {client_address}\n",
        "\n",
        "Instructions:\n",
        "1. Identify the specific country from the client information\n",
        "2. Determine the primary language used for news and media in that country\n",
        "3. Provide the appropriate Google News language and country codes\n",
        "\n",
        "Return ONLY a JSON object with this exact format:\n",
        "{{\n",
        "    \"lang\": \"language_code\",\n",
        "    \"country\": \"country_code\",\n",
        "    \"analysis\": \"Brief explanation of the determination\",\n",
        "    \"primary_language_name\": \"Language name\"\n",
        "}}\n",
        "\n",
        "Language Codes (ISO 639-1): en, es, zh, ja, ko, fr, de, it, pt, ru, ar, hi, th, vi, nl, sv, da, no, fi, pl, tr, etc.\n",
        "Country Codes (ISO 3166-1): US, MX, CN, JP, KR, TW, HK, SG, IN, MY, TH, VN, PH, AU, CA, GB, DE, FR, IT, ES, BR, AR, etc.\n",
        "\n",
        "Examples:\n",
        "- Mexico → {{\"lang\": \"es\", \"country\": \"MX\"}}\n",
        "- Taiwan → {{\"lang\": \"zh\", \"country\": \"TW\"}}\n",
        "- Thailand → {{\"lang\": \"th\", \"country\": \"TH\"}}\n",
        "- Netherlands → {{\"lang\": \"nl\", \"country\": \"NL\"}}\n",
        "- Singapore → {{\"lang\": \"en\", \"country\": \"SG\"}}\n",
        "\n",
        "Focus on the PRIMARY language used for news media in that country.\n",
        "\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a geographical and linguistic expert. Always respond with valid JSON only.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            # Make GPT API call\n",
        "            if self.use_legacy_api:\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=messages,\n",
        "                    max_tokens=300,\n",
        "                    temperature=0.1\n",
        "                )\n",
        "                result = response.choices[0].message.content.strip()\n",
        "            else:\n",
        "                response = self.openai_client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=messages,\n",
        "                    max_tokens=300,\n",
        "                    temperature=0.1\n",
        "                )\n",
        "                result = response.choices[0].message.content.strip()\n",
        "\n",
        "            # Clean and parse JSON\n",
        "            result = self._clean_json_response(result)\n",
        "            lang_data = json.loads(result)\n",
        "\n",
        "            lang_config = {\n",
        "                'lang': lang_data.get('lang', 'en').lower(),\n",
        "                'country': lang_data.get('country', 'US').upper()\n",
        "            }\n",
        "\n",
        "            print(f\"   🌍 GPT Language Analysis: {client_country} → {lang_config['lang']}-{lang_config['country']} ({lang_data.get('primary_language_name', 'Unknown')})\")\n",
        "\n",
        "            # Cache the result\n",
        "            self.country_lang_cache[cache_key] = lang_config\n",
        "            return lang_config\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ GPT language analysis failed for {client_country}: {e}\")\n",
        "\n",
        "            # Fallback to default mapping or English\n",
        "            fallback = DEFAULT_COUNTRY_LANG_MAP.get(client_upper, {'lang': 'en', 'country': 'US'})\n",
        "            self.country_lang_cache[cache_key] = fallback\n",
        "            return fallback\n",
        "\n",
        "    def localize_disaster_terms(self, english_terms: List[str], target_lang: str, target_country: str) -> List[str]:\n",
        "        \"\"\"Use GPT to translate disaster terms to local language and terminology\"\"\"\n",
        "\n",
        "        if target_lang == 'en' or not english_terms:\n",
        "            return english_terms\n",
        "\n",
        "        try:\n",
        "            terms_text = ', '.join(english_terms)\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "You are a disaster terminology localization expert. Translate these English disaster terms to the local language and terminology commonly used in {target_country}.\n",
        "\n",
        "English Terms: {terms_text}\n",
        "Target Language: {target_lang}\n",
        "Target Country: {target_country}\n",
        "\n",
        "Instructions:\n",
        "1. Translate each term to the local language commonly used in {target_country}\n",
        "2. Use the specific disaster terminology that local news media and authorities use\n",
        "3. Consider regional variations and preferred terms\n",
        "4. Maintain the disaster context and severity implications\n",
        "\n",
        "Examples of localization:\n",
        "- \"tropical cyclone\" in Mexico → \"huracán\"\n",
        "- \"tropical cyclone\" in Taiwan → \"颱風\" (typhoon)\n",
        "- \"earthquake\" in Japan → \"地震\"\n",
        "- \"flood\" in Germany → \"Hochwasser\"\n",
        "- \"wildfire\" in Spain → \"incendio forestal\"\n",
        "\n",
        "Return ONLY a JSON object:\n",
        "{{\n",
        "    \"localized_terms\": [\"term1\", \"term2\", \"term3\"],\n",
        "    \"translations\": {{\n",
        "        \"english_term1\": \"local_term1\",\n",
        "        \"english_term2\": \"local_term2\"\n",
        "    }}\n",
        "}}\n",
        "\n",
        "Focus on terms that would be used in news headlines and official communications in {target_country}.\n",
        "\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a disaster terminology localization expert. Always respond with valid JSON only.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            if self.use_legacy_api:\n",
        "                response = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=messages,\n",
        "                    max_tokens=400,\n",
        "                    temperature=0.2\n",
        "                )\n",
        "                result = response.choices[0].message.content.strip()\n",
        "            else:\n",
        "                response = self.openai_client.chat.completions.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=messages,\n",
        "                    max_tokens=400,\n",
        "                    temperature=0.2\n",
        "                )\n",
        "                result = response.choices[0].message.content.strip()\n",
        "\n",
        "            result = self._clean_json_response(result)\n",
        "            localization_data = json.loads(result)\n",
        "\n",
        "            localized_terms = localization_data.get('localized_terms', english_terms)\n",
        "            translations = localization_data.get('translations', {})\n",
        "\n",
        "            print(f\"   🌐 Localized terms for {target_country}: {', '.join(localized_terms[:3])}...\")\n",
        "            if translations:\n",
        "                print(f\"   📝 Key translations: {dict(list(translations.items())[:2])}\")\n",
        "\n",
        "            return localized_terms\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ⚠️ Term localization failed: {e}, using English terms\")\n",
        "            return english_terms\n",
        "\n",
        "    def _clean_json_response(self, response: str) -> str:\n",
        "      \"\"\"Clean GPT response to extract valid JSON with robust error handling\"\"\"\n",
        "      try:\n",
        "          # Remove markdown code blocks\n",
        "          response = re.sub(r'```json\\s*', '', response, flags=re.IGNORECASE)\n",
        "          response = re.sub(r'```\\s*', '', response)\n",
        "\n",
        "          # Remove common prefixes/suffixes\n",
        "          response = re.sub(r'^[^{]*(?=\\{)', '', response)  # Remove everything before first {\n",
        "          response = re.sub(r'\\}[^}]*$', '}', response)     # Remove everything after last }\n",
        "\n",
        "          # Remove invisible characters\n",
        "          response = response.replace('\\ufeff', '').replace('\\u200b', '').replace('\\u200c', '').replace('\\u200d', '').replace('\\ufffe', '')\n",
        "\n",
        "          # Fix common JSON issues\n",
        "          # Remove trailing commas before } or ]\n",
        "          response = re.sub(r',(\\s*[}\\]])', r'\\1', response)\n",
        "          # Fix multiple commas\n",
        "          response = re.sub(r',\\s*,', ',', response)\n",
        "          # Remove commas at the end of the last property\n",
        "          response = re.sub(r',(\\s*}\\s*})', r'\\1', response)\n",
        "\n",
        "          # Find JSON object\n",
        "          json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "          if json_match:\n",
        "              response = json_match.group(0)\n",
        "\n",
        "          # Additional cleanup for specific patterns that cause parsing issues\n",
        "          # Remove trailing comma before closing brace in nested objects\n",
        "          response = re.sub(r',(\\s*}\\s*[,}])', r'\\1', response)\n",
        "\n",
        "          response = response.strip()\n",
        "\n",
        "          # Test if it's valid JSON\n",
        "          json.loads(response)\n",
        "\n",
        "          return response\n",
        "      except json.JSONDecodeError:\n",
        "          # If still invalid, try a more aggressive approach\n",
        "          try:\n",
        "              # Extract just the client_risk_scores part\n",
        "              scores_match = re.search(r'\"client_risk_scores\":\\s*\\{[^}]+\\}', response, re.DOTALL)\n",
        "              if scores_match:\n",
        "                  scores_part = scores_match.group(0)\n",
        "                  # Create a minimal valid JSON\n",
        "                  minimal_json = '{ ' + scores_part + ' }'\n",
        "                  json.loads(minimal_json)  # Test validity\n",
        "                  return minimal_json\n",
        "          except:\n",
        "              pass\n",
        "          return response.strip()\n",
        "      except:\n",
        "          return response.strip()\n",
        "\n",
        "# ==========================================\n",
        "# DATE PARSING UTILITIES (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "def parse_date_input(date_str: str) -> Optional[datetime]:\n",
        "    \"\"\"Parse user date input with multiple formats including ISO datetime.\"\"\"\n",
        "    if not date_str or pd.isna(date_str):\n",
        "        return None\n",
        "\n",
        "    date_str = str(date_str).strip()\n",
        "\n",
        "    date_formats = [\n",
        "        '%Y-%m-%dT%H:%M:%S',      # ISO format from Excel (PRIORITY)\n",
        "        '%Y-%m-%d',\n",
        "        '%Y/%m/%d',\n",
        "        '%d-%m-%Y',\n",
        "        '%d/%m/%Y',\n",
        "        '%m-%d-%Y',\n",
        "        '%m/%d/%Y',\n",
        "        '%Y-%m-%d %H:%M:%S',\n",
        "        '%Y/%m/%d %H:%M:%S',\n",
        "    ]\n",
        "\n",
        "    for fmt in date_formats:\n",
        "        try:\n",
        "            return datetime.strptime(date_str, fmt)\n",
        "        except ValueError:\n",
        "            continue\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_date_range_from_excel_row(gdacs_event) -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"Extract and format date ranges from Excel row data.\"\"\"\n",
        "    try:\n",
        "        from_date = None\n",
        "        to_date = None\n",
        "\n",
        "        print(f\"   📋 Raw Excel dates - From_Date: '{gdacs_event.from_date}', To_Date: '{gdacs_event.to_date}'\")\n",
        "\n",
        "        # Parse From_Date\n",
        "        if hasattr(gdacs_event, 'from_date') and gdacs_event.from_date:\n",
        "            from_dt = parse_date_input(gdacs_event.from_date)\n",
        "            if from_dt:\n",
        "                from_date = from_dt.strftime('%Y-%m-%d')\n",
        "                print(f\"   ✅ Successfully parsed From_Date: {gdacs_event.from_date} -> {from_date}\")\n",
        "            else:\n",
        "                print(f\"   ❌ Failed to parse From_Date: '{gdacs_event.from_date}'\")\n",
        "        else:\n",
        "            print(f\"   ⚠️ No From_Date found in Excel data\")\n",
        "\n",
        "        # Parse To_Date\n",
        "        if hasattr(gdacs_event, 'to_date') and gdacs_event.to_date:\n",
        "            to_dt = parse_date_input(gdacs_event.to_date)\n",
        "            if to_dt:\n",
        "                to_date = to_dt.strftime('%Y-%m-%d')\n",
        "                print(f\"   ✅ Successfully parsed To_Date: {gdacs_event.to_date} -> {to_date}\")\n",
        "            else:\n",
        "                print(f\"   ❌ Failed to parse To_Date: '{gdacs_event.to_date}'\")\n",
        "        else:\n",
        "            print(f\"   ⚠️ No To_Date found in Excel data\")\n",
        "\n",
        "        # If no dates are provided, use the default range (the past 7 days).\n",
        "        if not from_date or not to_date:\n",
        "            end_date = datetime.now()\n",
        "            start_date = end_date - timedelta(days=7)\n",
        "            from_date = start_date.strftime('%Y-%m-%d')\n",
        "            to_date = end_date.strftime('%Y-%m-%d')\n",
        "            print(f\"   📅 Using default date range (parsing failed): {from_date} to {to_date}\")\n",
        "        else:\n",
        "            print(f\"   📅 Using Excel date range: {from_date} to {to_date}\")\n",
        "\n",
        "        return from_date, to_date\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   ⚠️ Date extraction error: {e}, using default range\")\n",
        "\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=7)\n",
        "        return start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d')\n",
        "\n",
        "# ==========================================\n",
        "# DATA CLASSES (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "@dataclass\n",
        "class GDACSEvent:\n",
        "    \"\"\"GDACS event information extracted from Excel data\"\"\"\n",
        "    event_id: str\n",
        "    episode_id: str\n",
        "    event_type: str\n",
        "    event_name: str\n",
        "    event_full_name: str\n",
        "    event_description: str\n",
        "    from_date: str\n",
        "    to_date: str\n",
        "    disaster_country: str\n",
        "    alert_level: str\n",
        "    alert_score: float\n",
        "    episode_alert_level: str\n",
        "    episode_alert_score: float\n",
        "    severity_value: float\n",
        "    severity_text: str\n",
        "    client_country: str\n",
        "    client_address: str\n",
        "    distance_km: float\n",
        "\n",
        "@dataclass\n",
        "class ClientInfo:\n",
        "    \"\"\"Client information from Detailed_Matches\"\"\"\n",
        "    event_id: str\n",
        "    client_country: str\n",
        "    severity_text: str\n",
        "    client_distance_summary: str\n",
        "    client_address: str\n",
        "\n",
        "@dataclass\n",
        "class DisasterAnalysis:\n",
        "    \"\"\"Enhanced disaster analysis results with GDACS-specific understanding\"\"\"\n",
        "    gpt_disaster_keywords: List[str] = field(default_factory=list)\n",
        "    gpt_response_keywords: List[str] = field(default_factory=list)\n",
        "    gpt_timeliness_keywords: List[str] = field(default_factory=list)\n",
        "    disaster_severity_score: float = 0.0\n",
        "    response_capability_score: float = 0.0\n",
        "    response_timeliness_score: float = 0.0\n",
        "    disaster_relevance_score: float = 0.0\n",
        "    gdacs_alignment_score: float = 0.0\n",
        "    media_coverage_score: float = 0.0\n",
        "    is_disaster_related: bool = False\n",
        "    severity_level: str = \"\"\n",
        "    response_level: str = \"\"\n",
        "    timeliness_level: str = \"\"\n",
        "    relevance_level: str = \"\"\n",
        "    gdacs_alignment_level: str = \"\"\n",
        "    media_coverage_level: str = \"\"\n",
        "    primary_disaster_type: str = \"\"\n",
        "    countries_mentioned: List[str] = field(default_factory=list)\n",
        "    detailed_locations: List[str] = field(default_factory=list)\n",
        "    gpt_analysis_summary: str = \"\"\n",
        "    gdacs_context_analysis: str = \"\"\n",
        "    client_country_match: bool = False\n",
        "    final_risk_score: float = 0.0\n",
        "    confidence_score: float = 0.0\n",
        "    # Client risk scoring fields\n",
        "    client_risk_scores: Dict[str, float] = field(default_factory=dict)\n",
        "    matched_client_addresses: List[str] = field(default_factory=list)\n",
        "    matched_location: str = \"\"\n",
        "\n",
        "@dataclass\n",
        "class MediaItem:\n",
        "    \"\"\"Media item data structure\"\"\"\n",
        "    title: str\n",
        "    url: str\n",
        "    date_text: str\n",
        "    parsed_date: Optional[datetime]\n",
        "    summary: str = \"\"\n",
        "    disaster_analysis: Optional[DisasterAnalysis] = None\n",
        "    gdacs_event: Optional[GDACSEvent] = None\n",
        "\n",
        "# ==========================================\n",
        "# DISASTER TYPE AND GDACS KNOWLEDGE BASE (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "DISASTER_TYPE_MAPPING = {\n",
        "    'EQ': 'earthquake',\n",
        "    'FL': 'flood',\n",
        "    'TC': 'hurricane',\n",
        "    'ST': 'storm',\n",
        "    'DR': 'drought',\n",
        "    'WF': 'wildfire',\n",
        "    'VO': 'volcano',\n",
        "    'LS': 'landslide',\n",
        "    'TS': 'tsunami',\n",
        "    'TO': 'tornado',\n",
        "    'AV': 'avalanche',\n",
        "    'CY': 'cyclone',\n",
        "    'BL': 'blizzard',\n",
        "    'HW': 'heatwave',\n",
        "    'HU': 'hurricane',\n",
        "    'TY': 'typhoon',\n",
        "    'TH': 'thunderstorm'\n",
        "}\n",
        "\n",
        "GDACS_SCORING_KNOWLEDGE = {\n",
        "    'EQ': {\n",
        "                'name': 'Earthquake',\n",
        "                'models': ['Shakemap Model', 'EQ Parameters Model'],\n",
        "                'key_inputs': ['MMI intensity', 'magnitude', 'depth', 'exposed population', 'vulnerability', 'INFORM LCC'],\n",
        "                'scoring_method': 'Shakemap intensity (MMI), exposed population, vulnerability, INFORM Lack of Coping Capacity',\n",
        "                'alert_thresholds': {'RED': '≥2.0', 'ORANGE': '1.0-2.0', 'GREEN': '<1.0'},\n",
        "                'key_factors': ['magnitude', 'depth', 'population_exposure', 'MMI_intensity', 'vulnerability', 'coping_capacity'],\n",
        "                'scoring_logic': {\n",
        "                    'condition': 'If max MMI in populated area ≤ VI → Alert = GREEN (Score = 0)',\n",
        "                    'scaled_population': '10×Pop(MMI IX) + Pop(MMI VIII) + 0.1×Pop(MMI VII)',\n",
        "                    'raw_score_formula': '–0.59 + 0.53 × log₁₀(Scaled Population)',\n",
        "                    'final_score': 'Shakemap Score × INFORM LCC',\n",
        "                    'special_rule': 'If Shakemap Score >2 but LCC reduces below 1, set final to 1'\n",
        "                },\n",
        "                'impact_indicators': ['casualties', 'building_damage', 'infrastructure_damage', 'MMI_levels', 'population_affected', 'aftershocks'],\n",
        "                'assessment_focus': 'How many people were strongly shaken? MMI IX (×10), VIII (×1), VII (×0.1)'\n",
        "            },\n",
        "            'TS': {\n",
        "                'name': 'Tsunami',\n",
        "                'models': ['Tsunami Wave Height Model'],\n",
        "                'key_inputs': ['earthquake magnitude (>6.5)', 'depth', 'water depth', 'modeled wave height'],\n",
        "                'scoring_method': 'Maximum wave height at coast, historical tsunami scenarios',\n",
        "                'alert_thresholds': {'RED': 'wave ≥3m (≥2.0)', 'ORANGE': 'wave 1-3m (1.0-2.0)', 'GREEN': 'wave <1m'},\n",
        "                'key_factors': ['wave_height', 'coastal_population', 'earthquake_magnitude', 'travel_time', 'coastal_topography'],\n",
        "                'scoring_logic': {\n",
        "                    'red_threshold': 'wave height ≥ 3m (score ≥ 2)',\n",
        "                    'orange_threshold': '1–3m (1 ≤ score < 2)',\n",
        "                    'green_threshold': '< 1m',\n",
        "                    'fallback': 'If no precomputed scenario, use magnitude-based IOC matrix'\n",
        "                },\n",
        "                'impact_indicators': ['wave_height', 'coastal_inundation', 'evacuation_areas', 'travel_time', 'coastal_damage', 'maritime_disruption'],\n",
        "                'assessment_focus': 'Wave height and coastal population exposure'\n",
        "            },\n",
        "            'TC': {\n",
        "                'name': 'Tropical Cyclone',\n",
        "                'models': ['Wind Impact Model', 'Storm Surge Model'],\n",
        "                'key_inputs': ['wind speed (1-min sustained)', 'Saffir-Simpson category', 'population exposure', 'vulnerability (HDI + rural)'],\n",
        "                'scoring_method': 'Wind impact zones, Saffir-Simpson scale, population exposure, HDI vulnerability',\n",
        "                'alert_thresholds': {'RED': 'Cat 3+ with high exposure', 'ORANGE': 'Cat 1-2 with significant exposure', 'GREEN': 'TS <10M people'},\n",
        "                'key_factors': ['wind_speed', 'category', 'population_exposure', 'storm_surge', 'vulnerability', 'track'],\n",
        "                'scoring_logic': {\n",
        "                    'green': 'TS affecting < 10M people (all vulnerability levels)',\n",
        "                    'orange': 'TS >10M high vuln; Cat 1-2 >100K/10% med-high vuln; Cat 3 >1M low vuln',\n",
        "                    'red': 'Cat 1-2 >1M high vuln; Cat 3 >100K/10% med-high vuln; Cat 4 >1M low vuln',\n",
        "                    'storm_surge': 'RED ≥3m; ORANGE 1-3m; GREEN ≤1m (calculated but not in overall score yet)'\n",
        "                },\n",
        "                'impact_indicators': ['wind_damage', 'storm_surge_height', 'flooding', 'evacuation_numbers', 'infrastructure_damage', 'landfall_intensity'],\n",
        "                'assessment_focus': 'Wind impact zones and population vulnerability'\n",
        "            },\n",
        "            'FL': {\n",
        "                'name': 'Flood',\n",
        "                'models': ['Impact-based Assessment'],\n",
        "                'key_inputs': ['reported deaths', 'displaced people', 'affected area', 'duration', 'official sources'],\n",
        "                'scoring_method': 'Impact-based assessment, reported casualties and displacement',\n",
        "                'alert_thresholds': {'RED': '>1000 deaths or >800K displaced', 'ORANGE': '>100 deaths or >80K displaced', 'GREEN': 'Other floods'},\n",
        "                'key_factors': ['deaths', 'displaced_people', 'affected_area', 'duration', 'infrastructure_damage', 'economic_impact'],\n",
        "                'scoring_logic': {\n",
        "                    'red_criteria': '>1,000 deaths OR >800,000 displaced',\n",
        "                    'orange_criteria': '>100 deaths OR >80,000 displaced',\n",
        "                    'green_criteria': 'All other floods',\n",
        "                    'trigger_method': 'Based on reported impacts rather than automated modeling'\n",
        "                },\n",
        "                'impact_indicators': ['casualties', 'displaced_population', 'flooded_area', 'infrastructure_damage', 'economic_loss', 'rescue_operations'],\n",
        "                'assessment_focus': 'Reported human impact and displacement numbers'\n",
        "            },\n",
        "            'VO': {\n",
        "                'name': 'Volcano',\n",
        "                'models': ['Volcanic Ash Advisory Model'],\n",
        "                'key_inputs': ['Volcanic Ash Advisories (VAAs)', 'ash plume height', 'eruption size', 'population proximity'],\n",
        "                'scoring_method': 'Volcanic Ash Advisories, eruption magnitude, populated area proximity',\n",
        "                'alert_thresholds': {'RED': 'Major international attention', 'ORANGE': 'Significant activity', 'GREEN': 'VAA detected activity'},\n",
        "                'key_factors': ['ash_plume_height', 'eruption_size', 'population_proximity', 'aviation_impact', 'ash_distribution'],\n",
        "                'scoring_logic': {\n",
        "                    'green': 'VAA red/orange or new activity detected (automatic)',\n",
        "                    'orange_red': 'Manually assigned for major volcanic events with international attention',\n",
        "                    'detection': 'Automatic via VAAs; manual interpretation for significant events'\n",
        "                },\n",
        "                'impact_indicators': ['ash_plume_height', 'aviation_disruption', 'population_evacuation', 'ash_fall', 'air_quality', 'international_attention'],\n",
        "                'assessment_focus': 'Aviation impact and international significance'\n",
        "            },\n",
        "            'DR': {\n",
        "                'name': 'Drought',\n",
        "                'models': ['RDrI-Agri Model'],\n",
        "                'key_inputs': ['RDrI-Agri index', 'agricultural risk', 'socio-economic factors', 'duration ≥1 month'],\n",
        "                'scoring_method': 'RDrI-Agri index, agricultural risk, socio-economic factors',\n",
        "                'alert_thresholds': {'RED': 'Life-threatening impacts', 'ORANGE': 'Economic impacts', 'GREEN': 'Mild/localized'},\n",
        "                'key_factors': ['duration', 'affected_area', 'agricultural_impact', 'food_security', 'economic_impact', 'coping_capacity'],\n",
        "                'scoring_logic': {\n",
        "                    'green': '0.25–0.5: Mild, localized, no impacts or high coping capacity',\n",
        "                    'orange': '0.75–1.75: Relevant economic/sectoral impacts, media coverage',\n",
        "                    'red': '2.0–3.0: Life-threatening—displacement, famine, international aid',\n",
        "                    'validation': 'Expert review with independent sources for impact confirmation'\n",
        "                },\n",
        "                'impact_indicators': ['crop_yield', 'food_security', 'livestock_impact', 'water_scarcity', 'economic_loss', 'migration'],\n",
        "                'assessment_focus': 'Agricultural impact and food security implications'\n",
        "            },\n",
        "            'WF': {\n",
        "                'name': 'Wildfire',\n",
        "                'models': ['Burnt Area and Population Proximity Model'],\n",
        "                'key_inputs': ['MODIS/VIIRS satellite data', 'burned area', 'population within 5km', 'casualties'],\n",
        "                'scoring_method': 'Burned area, population within 5km, casualties, infrastructure damage',\n",
        "                'alert_thresholds': {'RED': '2.5 (severe impact)', 'ORANGE': '1.5', 'GREEN': '0.5 (auto)'},\n",
        "                'key_factors': ['burned_area', 'population_proximity', 'fatalities', 'infrastructure_damage', 'evacuation', 'coping_capacity'],\n",
        "                'scoring_logic': {\n",
        "                    'detection': 'Automatic when burned area ≥5,000 ha; manual if smaller but near population',\n",
        "                    'display': 'Displayed if ≥10,000 ha burned AND ≥10,000 people within 5km',\n",
        "                    'red_criteria': 'Severe impact: displacements, fatalities, UCPM activation, low coping capacity'\n",
        "                },\n",
        "                'impact_indicators': ['burned_area', 'casualties', 'displaced_population', 'property_damage', 'air_quality', 'evacuation_scale'],\n",
        "                'assessment_focus': 'Burned area scale and population proximity'\n",
        "            }\n",
        "}\n",
        "\n",
        "ALERT_LEVEL_DEFINITIONS = {\n",
        "    'RED': {\n",
        "        'severity': 'High',\n",
        "        'definition': 'Very likely humanitarian impact',\n",
        "        'description': 'Humanitarian impact is very likely, significant international assistance may be required',\n",
        "        'action_required': 'Immediate response and international assistance needed'\n",
        "    },\n",
        "    'ORANGE': {\n",
        "        'severity': 'Medium',\n",
        "        'definition': 'Humanitarian impact possible',\n",
        "        'description': 'Humanitarian impact is possible, affected country response capabilities should be monitored',\n",
        "        'action_required': 'Monitor closely and prepare response resources'\n",
        "    },\n",
        "    'GREEN': {\n",
        "        'severity': 'Low',\n",
        "        'definition': 'No or minimal humanitarian impact',\n",
        "        'description': 'No significant humanitarian impact expected, but event is being monitored',\n",
        "        'action_required': 'Continue monitoring situation'\n",
        "    }\n",
        "}\n",
        "\n",
        "# ==========================================\n",
        "# EXCEL DATA PROCESSOR CLASS (UNCHANGED)\n",
        "# ==========================================\n",
        "\n",
        "class OptimizedExcelProcessor:\n",
        "    \"\"\"Optimized Excel processor that supports loading customer information.\"\"\"\n",
        "\n",
        "    def __init__(self, excel_file_path=\"enhanced_disaster_analysis_400.0km_buffer10.0km 2.xlsx\"):\n",
        "        self.excel_file_path = excel_file_path\n",
        "        self.disaster_data = None\n",
        "        self.client_info_by_event = {}\n",
        "        self.client_countries_by_event = {}\n",
        "        self.load_data()\n",
        "        self.load_client_info()\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"Load disaster data from the Detailed_Matches sheet\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(self.excel_file_path):\n",
        "                print(f\"❌ Excel file not found: {self.excel_file_path}\")\n",
        "                return\n",
        "\n",
        "            # Load the Detailed_Matches sheet directly\n",
        "            self.disaster_data = pd.read_excel(self.excel_file_path, sheet_name=\"Detailed_Matches\")\n",
        "            print(f\"📊 Successfully loaded {len(self.disaster_data)} rows from Detailed_Matches sheet\")\n",
        "            print(f\"📋 Key columns found: Event_ID, Event_Type, Event_Name, Alert_Level, Client_Country, etc.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error loading Excel data: {str(e)}\")\n",
        "            self.disaster_data = None\n",
        "\n",
        "    def load_client_info(self):\n",
        "        \"\"\"Load complete client information from Detailed_Matches sheet\"\"\"\n",
        "        try:\n",
        "            logger.info(\"📊 Loading client information from Detailed_Matches sheet\")\n",
        "\n",
        "            if self.disaster_data is None:\n",
        "                logger.warning(\"No disaster data available for client info loading\")\n",
        "                return\n",
        "\n",
        "            required_columns = ['Event_ID', 'Client_Country', 'Severity_Text', 'Client_Distance_Summary', 'Client_Address']\n",
        "            missing_columns = [col for col in required_columns if col not in self.disaster_data.columns]\n",
        "\n",
        "            if missing_columns:\n",
        "                logger.warning(f\"Missing required columns for client info: {missing_columns}\")\n",
        "                return\n",
        "\n",
        "            # Process each row to extract client information\n",
        "            for idx, row in self.disaster_data.iterrows():\n",
        "                try:\n",
        "                    event_id = self.safe_string_conversion(row.get('Event_ID', ''))\n",
        "                    if not event_id:\n",
        "                        continue\n",
        "\n",
        "                    client_info = ClientInfo(\n",
        "                        event_id=event_id,\n",
        "                        client_country=self.safe_string_conversion(row.get('Client_Country', '')),\n",
        "                        severity_text=self.safe_string_conversion(row.get('Severity_Text', '')),\n",
        "                        client_distance_summary=self.safe_string_conversion(row.get('Client_Distance_Summary', '')),\n",
        "                        client_address=self.safe_string_conversion(row.get('Client_Address', ''))\n",
        "                    )\n",
        "\n",
        "                    if event_id not in self.client_info_by_event:\n",
        "                        self.client_info_by_event[event_id] = []\n",
        "                        self.client_countries_by_event[event_id] = []\n",
        "\n",
        "                    self.client_info_by_event[event_id].append(client_info)\n",
        "\n",
        "                    # Also maintain the old client_countries_by_event for compatibility\n",
        "                    if client_info.client_country not in self.client_countries_by_event[event_id]:\n",
        "                        self.client_countries_by_event[event_id].append(client_info.client_country)\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error processing client info for row {idx}: {e}\")\n",
        "                    continue\n",
        "\n",
        "            total_events_with_clients = len(self.client_info_by_event)\n",
        "            total_client_records = sum(len(clients) for clients in self.client_info_by_event.values())\n",
        "\n",
        "            logger.info(f\"✅ Loaded client information for {total_events_with_clients} events\")\n",
        "            logger.info(f\"📋 Total client records: {total_client_records}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Loading client information failed: {e}\")\n",
        "            logger.info(\"Continuing without client information...\")\n",
        "\n",
        "    def safe_string_conversion(self, value, default=''):\n",
        "        \"\"\"Safe string conversion\"\"\"\n",
        "        if pd.isna(value) or value is None:\n",
        "            return default\n",
        "        return str(value).strip()\n",
        "\n",
        "    def get_row_data(self, row_index: int) -> Optional[GDACSEvent]:\n",
        "        \"\"\"Extract GDACS event data from a specific row\"\"\"\n",
        "        if self.disaster_data is None or row_index >= len(self.disaster_data):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            row = self.disaster_data.iloc[row_index]\n",
        "\n",
        "            # Extract and clean data with proper handling of NaN values\n",
        "            def safe_str(value, default=''):\n",
        "                if pd.isna(value) or value is None:\n",
        "                    return default\n",
        "                return str(value).strip()\n",
        "\n",
        "            def safe_float(value, default=0.0):\n",
        "                if pd.isna(value) or value is None:\n",
        "                    return default\n",
        "                try:\n",
        "                    return float(value)\n",
        "                except (ValueError, TypeError):\n",
        "                    return default\n",
        "\n",
        "            # Map event type to readable format\n",
        "            event_type_raw = safe_str(row.get('Event_Type', ''))\n",
        "            event_type = DISASTER_TYPE_MAPPING.get(event_type_raw.upper(), event_type_raw.lower())\n",
        "\n",
        "            # Normalize alert level\n",
        "            alert_level = safe_str(row.get('Alert_Level', '')).upper()\n",
        "            if alert_level not in ['RED', 'ORANGE', 'GREEN']:\n",
        "                if alert_level.lower() in ['high', 'severe', '3']:\n",
        "                    alert_level = 'RED'\n",
        "                elif alert_level.lower() in ['medium', 'moderate', '2']:\n",
        "                    alert_level = 'ORANGE'\n",
        "                else:\n",
        "                    alert_level = 'GREEN'\n",
        "\n",
        "            return GDACSEvent(\n",
        "                event_id=safe_str(row.get('Event_ID', f\"event_{row_index}\")),\n",
        "                episode_id=safe_str(row.get('Episode_ID', '')),\n",
        "                event_type=event_type,\n",
        "                event_name=safe_str(row.get('Event_Name', f'Event {row_index + 1}')),\n",
        "                event_full_name=safe_str(row.get('Event_Full_Name', '')),\n",
        "                event_description=safe_str(row.get('Event_Description', ''))[:500],\n",
        "                from_date=safe_str(row.get('From_Date', '')),\n",
        "                to_date=safe_str(row.get('To_Date', '')),\n",
        "                disaster_country=safe_str(row.get('Disaster_Country', '')),\n",
        "                alert_level=alert_level,\n",
        "                alert_score=safe_float(row.get('Alert_Score', 1.0)),\n",
        "                episode_alert_level=safe_str(row.get('Episode_Alert_Level', alert_level)),\n",
        "                episode_alert_score=safe_float(row.get('Episode_Alert_Score', 1.0)),\n",
        "                severity_value=safe_float(row.get('Severity_Value', 0.0)),\n",
        "                severity_text=safe_str(row.get('Severity_Text', '')),\n",
        "                client_country=safe_str(row.get('Client_Country', '')),\n",
        "                client_address=safe_str(row.get('Client_Address', '')),\n",
        "                distance_km=safe_float(row.get('Distance_KM', 0.0))\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing row {row_index}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_total_rows(self):\n",
        "        \"\"\"Get total number of rows available for processing\"\"\"\n",
        "        return len(self.disaster_data) if self.disaster_data is not None else 0\n",
        "\n",
        "    def get_client_countries_for_event(self, event_id: str) -> List[str]:\n",
        "        \"\"\"Get client countries for a specific Event_ID\"\"\"\n",
        "        return self.client_countries_by_event.get(event_id, [])\n",
        "\n",
        "    def get_client_info_for_event(self, event_id: str) -> List[ClientInfo]:\n",
        "        \"\"\"Get client info for a specific Event_ID\"\"\"\n",
        "        return self.client_info_by_event.get(event_id, [])\n",
        "\n",
        "# ==========================================\n",
        "# PYGOOGLENEWS CLIENT WITH LOCALIZATION (NEW)\n",
        "# ==========================================\n",
        "\n",
        "class LocalizedPyGoogleNewsClient:\n",
        "    \"\"\"Enhanced PyGoogleNews client with localization and date filtering\"\"\"\n",
        "\n",
        "    def __init__(self, lang='en', country='US'):\n",
        "        self.lang = lang.lower()\n",
        "        self.country = country.upper()\n",
        "        try:\n",
        "            self.gn = GoogleNews(lang=self.lang, country=self.country)\n",
        "            print(f\"✅ PyGoogleNews initialized with lang={self.lang}, country={self.country}\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ PyGoogleNews initialization failed: {e}\")\n",
        "            # Fallback to default\n",
        "            self.gn = GoogleNews(lang='en', country='US')\n",
        "            print(f\"✅ PyGoogleNews fallback to lang=en, country=US\")\n",
        "\n",
        "    def reinitialize_for_country(self, lang: str, country: str):\n",
        "        \"\"\"Reinitialize GoogleNews with new language and country settings\"\"\"\n",
        "        try:\n",
        "            self.lang = lang.lower()\n",
        "            self.country = country.upper()\n",
        "            self.gn = GoogleNews(lang=self.lang, country=self.country)\n",
        "            print(f\"🌍 PyGoogleNews reinitialized for lang={self.lang}, country={self.country}\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"❌ PyGoogleNews reinitialization failed: {e}\")\n",
        "            return False\n",
        "\n",
        "    def _parse_pubdate(self, entry) -> datetime:\n",
        "        \"\"\"Convert the entry's published field to datetime\"\"\"\n",
        "        try:\n",
        "            if hasattr(entry, 'published') and entry.published:\n",
        "                # Try parsing various formats\n",
        "                pub_date = entry.published\n",
        "\n",
        "                # Handle different date formats from PyGoogleNews\n",
        "                for fmt in ['%a, %d %b %Y %H:%M:%S %Z',\n",
        "                           '%Y-%m-%dT%H:%M:%SZ',\n",
        "                           '%Y-%m-%d %H:%M:%S',\n",
        "                           '%d %b %Y %H:%M:%S']:\n",
        "                    try:\n",
        "                        return datetime.strptime(pub_date, fmt)\n",
        "                    except ValueError:\n",
        "                        continue\n",
        "\n",
        "                # Fallback parsing\n",
        "                from email.utils import parsedate_to_datetime\n",
        "                return parsedate_to_datetime(pub_date).replace(tzinfo=None)\n",
        "\n",
        "            return None\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "    def _within_range(self, dt: datetime, start_dt: datetime, end_dt: datetime) -> bool:\n",
        "        \"\"\"Check if datetime is within range\"\"\"\n",
        "        if dt is None:\n",
        "            return False\n",
        "        return start_dt <= dt <= end_dt\n",
        "\n",
        "    def _to_dt(self, s, default=None):\n",
        "        \"\"\"Convert date string to datetime\"\"\"\n",
        "        if not s:\n",
        "            return default\n",
        "        for fmt in ('%Y-%m-%d', '%Y/%m/%d'):\n",
        "            try:\n",
        "                return datetime.strptime(s, fmt)\n",
        "            except ValueError:\n",
        "                continue\n",
        "        for fmt in ('%Y-%m-%d %H:%M:%S', '%Y/%m/%d %H:%M:%S'):\n",
        "            try:\n",
        "                return datetime.strptime(s, fmt)\n",
        "            except ValueError:\n",
        "                continue\n",
        "        return default\n",
        "\n",
        "    def search(self, query: str, max_results=20, from_=None, to_=None):\n",
        "        \"\"\"Search with PyGoogleNews and apply date filtering\"\"\"\n",
        "        try:\n",
        "            if not query.strip():\n",
        "                return {'entries': [], 'status': 'error', 'message': 'Empty query'}\n",
        "\n",
        "            # Prepare date bounds\n",
        "            now = datetime.now()\n",
        "            start_dt = self._to_dt(from_, datetime(1900, 1, 1))\n",
        "            end_dt = self._to_dt(to_, now)\n",
        "            if end_dt > now:\n",
        "                end_dt = now\n",
        "            if start_dt > end_dt:\n",
        "                start_dt, end_dt = end_dt, end_dt\n",
        "\n",
        "            print(f\"      Date filtering: {start_dt.strftime('%Y-%m-%d')} to {end_dt.strftime('%Y-%m-%d')}\")\n",
        "\n",
        "            # Perform search with PyGoogleNews\n",
        "            search_results = self.gn.search(query)\n",
        "\n",
        "            processed_entries = []\n",
        "            total_entries = 0\n",
        "            filtered_out = 0\n",
        "\n",
        "            for entry in search_results['entries']:\n",
        "                total_entries += 1\n",
        "\n",
        "                # Parse publication date and apply local filtering\n",
        "                pub_dt = self._parse_pubdate(entry)\n",
        "                if not self._within_range(pub_dt, start_dt, end_dt):\n",
        "                    filtered_out += 1\n",
        "                    continue\n",
        "\n",
        "                # Process entry content\n",
        "                title = self._clean_html(getattr(entry, 'title', '') or '')\n",
        "                summary = self._clean_html(getattr(entry, 'summary', '') or '')\n",
        "                link = getattr(entry, 'link', '') or ''\n",
        "                published = getattr(entry, 'published', '') or ''\n",
        "\n",
        "                source = 'Unknown'\n",
        "                if hasattr(entry, 'source') and entry.source:\n",
        "                    source = getattr(entry.source, 'title', 'Unknown')\n",
        "\n",
        "                if not title or not link or len(title.strip()) < 5:\n",
        "                    continue\n",
        "\n",
        "                processed_entries.append({\n",
        "                    'title': title[:300],\n",
        "                    'summary': summary[:500],\n",
        "                    'link': link,\n",
        "                    'published': published,\n",
        "                    'parsed_date': pub_dt.strftime('%Y-%m-%d %H:%M:%S') if pub_dt else '',\n",
        "                    'source': source[:100]\n",
        "                })\n",
        "\n",
        "                if len(processed_entries) >= max_results:\n",
        "                    break\n",
        "\n",
        "            print(f\"      PyGoogleNews entries: {total_entries}, Filtered out: {filtered_out}, Selected: {len(processed_entries)}\")\n",
        "\n",
        "            return {\n",
        "                'entries': processed_entries,\n",
        "                'status': 'ok',\n",
        "                'total_results': len(processed_entries),\n",
        "                'query': query,\n",
        "                'search_query': query,\n",
        "                'date_filter_stats': {\n",
        "                    'total_entries': total_entries,\n",
        "                    'filtered_out': filtered_out,\n",
        "                    'selected': len(processed_entries)\n",
        "                },\n",
        "                'debug_query': query,\n",
        "                'lang_country': f\"{self.lang}-{self.country}\"\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            return {'entries': [], 'status': 'error', 'message': str(e)}\n",
        "\n",
        "    def _clean_html(self, text: str) -> str:\n",
        "        \"\"\"Clean HTML tags and normalize text\"\"\"\n",
        "        if not text or pd.isna(text):\n",
        "            return \"\"\n",
        "\n",
        "        try:\n",
        "            text = html.unescape(str(text))\n",
        "            soup = BeautifulSoup(text, 'html.parser')\n",
        "            clean_text = soup.get_text()\n",
        "            clean_text = re.sub(r'\\s+', ' ', clean_text).strip()\n",
        "            return clean_text\n",
        "        except Exception:\n",
        "            return str(text).strip() if text else \"\"\n",
        "\n",
        "# ==========================================\n",
        "# GPT URL CONTENT ANALYZER\n",
        "# ==========================================\n",
        "\n",
        "class GPTURLAnalyzer:\n",
        "    \"\"\"GPT-based URL content analyzer to replace BeautifulSoup web scraping\"\"\"\n",
        "\n",
        "    def __init__(self, openai_client, use_legacy_api=False):\n",
        "        self.openai_client = openai_client\n",
        "        self.use_legacy_api = use_legacy_api\n",
        "        self.session = requests.Session()\n",
        "        self.session.headers.update({\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
        "        })\n",
        "\n",
        "    def extract_url_content_with_gpt(self, url: str, title: str = \"\", timeout: int = 20) -> Dict[str, str]:\n",
        "        \"\"\"\n",
        "        Use GPT to analyze and extract comprehensive content from a URL.\n",
        "        Returns dict with 'content', 'summary', and 'status' keys.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not url or url == 'No URL':\n",
        "                return {\n",
        "                    'content': title,\n",
        "                    'summary': title[:300] if title else \"No content available\",\n",
        "                    'status': 'no_url'\n",
        "                }\n",
        "\n",
        "            print(f\"            🤖 GPT analyzing URL for comprehensive content: {url[:60]}...\")\n",
        "\n",
        "            # First, fetch the raw HTML\n",
        "            try:\n",
        "                response = self.session.get(url, timeout=timeout)\n",
        "                if response.status_code != 200:\n",
        "                    print(f\"            ❌ HTTP {response.status_code} for URL\")\n",
        "                    return {\n",
        "                        'content': title,\n",
        "                        'summary': title[:300] if title else \"URL not accessible\",\n",
        "                        'status': 'http_error'\n",
        "                    }\n",
        "\n",
        "                # Get raw HTML (increased limit for better extraction)\n",
        "                raw_html = response.text[:15000]  # Increased from 8000 to 15000 chars\n",
        "\n",
        "            except Exception as fetch_error:\n",
        "                print(f\"            ❌ URL fetch failed: {fetch_error}\")\n",
        "                return {\n",
        "                    'content': title,\n",
        "                    'summary': title[:300] if title else \"URL fetch failed\",\n",
        "                    'status': 'fetch_error'\n",
        "                }\n",
        "\n",
        "            # Use GPT to extract comprehensive content\n",
        "            prompt = f\"\"\"\n",
        "You are an expert news article content extractor specialized in disaster and emergency news analysis. Your task is to extract comprehensive, detailed article content for thorough analysis.\n",
        "\n",
        "URL: {url}\n",
        "Article Title: {title}\n",
        "\n",
        "HTML Content:\n",
        "{raw_html}\n",
        "\n",
        "You are an expert news article content extractor specialized in disaster and emergency news analysis. Your task is to extract comprehensive, detailed article content for thorough analysis.\n",
        "\n",
        "URL: {url}\n",
        "Article Title: {title}\n",
        "\n",
        "HTML Content:\n",
        "{raw_html}\n",
        "\n",
        "Please extract and return ONLY a JSON object with this exact format:\n",
        "{{\n",
        "    \"main_content\": \"COMPREHENSIVE main article text content including ALL countries disruption happening, paragraphs, quotes, statistics, and details. Remove HTML tags, ads, navigation, but KEEP ALL article text including background information, casualty figures, response details, timeline information, expert quotes, and humanitarian impacts.\",\n",
        "    \"summary\": \"A detailed 13-15 sentence summary capturing disaster severity, human/economic impact, response measures, recovery efforts, and resilience factors. If certain categories are missing, explicitly state their absence in natural language (e.g., 'The article does not mention economic losses' or 'There is no information about school closures or preparedness drills').\",\n",
        "    \"extraction_success\": true/false,\n",
        "    \"content_length\": <number of words in main_content>,\n",
        "    \"key_details_found\": [\n",
        "        \"list of categories covered\",\n",
        "        \"plus natural notes for missing items (e.g., 'no information on economic losses')\"\n",
        "    ]\n",
        "}}\n",
        "\n",
        "---------------------------------------------------\n",
        "CATEGORY SET 1 – DISASTER SEVERITY & IMPACT\n",
        "(Focus on the seriousness of the disaster and its direct consequences)\n",
        "\n",
        "1. Casualties & Affected Population\n",
        "   - Deaths, injuries, missing persons, displaced people (exact numbers or estimates).\n",
        "\n",
        "2. Property & Infrastructure Damage\n",
        "   - Homes, schools, hospitals, businesses, transportation systems.\n",
        "\n",
        "3. Economic Losses\n",
        "   - Monetary estimates, industry-specific impacts.\n",
        "\n",
        "4. Public Service Disruptions\n",
        "   - School closures, work stoppages, transportation shutdowns, electricity/water/communication outages (with duration).\n",
        "\n",
        "5. Disaster Timeline\n",
        "   - Event onset, sequence of events, secondary hazards.\n",
        "\n",
        "6. Geographic & Contextual Background\n",
        "   - Locations, geography, comparisons with past disasters.\n",
        "\n",
        "---------------------------------------------------\n",
        "CATEGORY SET 2 – RESPONSE & RESILIENCE\n",
        "(Focus on government, community, and systemic response measures)\n",
        "\n",
        "1. Critical Infrastructure Functionality\n",
        "   - Hospitals, power, water, communication systems; speed of restoration.\n",
        "\n",
        "2. Government Crisis Management\n",
        "   - Effectiveness of response, staffing, leadership, emergency declarations.\n",
        "\n",
        "3. Evacuation & Preparedness Measures\n",
        "   - Evacuation orders, warning systems, drills, preparedness programs.\n",
        "\n",
        "4. Emergency Resources & Relief Efforts\n",
        "   - Food, water, fuel, medical resources, shelters, search-and-rescue operations.\n",
        "\n",
        "5. Coordination & Communication\n",
        "   - Agency cooperation, command structures, info sharing, international support.\n",
        "\n",
        "6. Learning from Past Disasters\n",
        "   - Building codes, land-use planning, retrofitted infrastructure, lessons applied.\n",
        "\n",
        "7. Economic Continuity & Recovery\n",
        "   - Essential services (banks, supply chains, utilities), business continuity, recovery capacity.\n",
        "\n",
        "8. Community & Volunteer Support\n",
        "   - Volunteer groups, NGOs, social/religious organizations.\n",
        "\n",
        "9. Risk Reduction & Public Education\n",
        "   - Protective infrastructure, public education, awareness campaigns.\n",
        "\n",
        "10. Specific Resilience Evidence\n",
        "   - Concrete examples (e.g., “backup generators kept hospitals running 72 hours,” “80% residents received SMS alerts”).\n",
        "\n",
        "---------------------------------------------------\n",
        "CRITICAL ENFORCEMENT\n",
        "- For EVERY category in both sets, either extract the actual information OR explicitly state its absence in natural language.\n",
        "- Do NOT leave categories blank. Always confirm presence or absence.\n",
        "- Be explicit and concrete. Example: “Three bridges collapsed” instead of “some damage occurred.”\n",
        "- Maintain structured completeness: all categories must be addressed.\n",
        "\n",
        "---------------------------------------------------\n",
        "EXAMPLE OUTPUT\n",
        "{{\n",
        "  \"main_content\": \"A magnitude 6.9 earthquake struck City Z, destroying hundreds of homes and cutting electricity for 48 hours. Officials confirmed 210 deaths and more than 1,000 injuries. Around 30,000 residents were displaced. The government declared a state of emergency, deploying 3,500 soldiers for search and rescue. Hospitals reported overcrowding, and relief camps were established in schools and stadiums. International aid agencies announced they were sending supplies. However, the article does not mention economic losses or details about preparedness drills.\",\n",
        "  \"summary\": \"A powerful earthquake hit City Z, causing 210 deaths, 1,000 injuries, and displacing 30,000 residents. Major damage occurred to housing and infrastructure, with power outages lasting 48 hours. The government deployed 3,500 soldiers, and shelters were opened in schools and stadiums. Hospitals struggled with capacity, while international aid groups mobilized assistance. The article does not mention economic losses. There is also no information about preparedness drills or resilience measures.\",\n",
        "  \"extraction_success\": true,\n",
        "  \"content_length\": 162,\n",
        "  \"key_details_found\": [\n",
        "    \"casualties (210 dead, 1,000 injured, 30,000 displaced)\",\n",
        "    \"infrastructure damage (housing destroyed, 48-hour power outage)\",\n",
        "    \"government response (emergency declared, 3,500 soldiers deployed)\",\n",
        "    \"relief efforts (shelters, international aid)\",\n",
        "    \"the article does not mention economic losses\",\n",
        "    \"no mention of preparedness drills or resilience measures\"\n",
        "  ]\n",
        "}}\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a comprehensive news content extraction expert focused on disaster analysis. Extract complete article content, not summaries. Prioritize thoroughness over brevity.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                # Make GPT API call with increased token limit for comprehensive extraction\n",
        "                if self.use_legacy_api:\n",
        "                    gpt_response = openai.ChatCompletion.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=messages,\n",
        "                        max_tokens=2000,  # Increased from 1000 to 2000\n",
        "                        temperature=0.1\n",
        "                    )\n",
        "                    result = gpt_response.choices[0].message.content.strip()\n",
        "                else:\n",
        "                    gpt_response = self.openai_client.chat.completions.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=messages,\n",
        "                        max_tokens=2000,  # Increased from 1000 to 2000\n",
        "                        temperature=0.1\n",
        "                    )\n",
        "                    result = gpt_response.choices[0].message.content.strip()\n",
        "\n",
        "                # Clean and parse JSON response\n",
        "                result = self._clean_json_response(result)\n",
        "\n",
        "                try:\n",
        "                    content_data = json.loads(result)\n",
        "\n",
        "                    if content_data.get('extraction_success', False):\n",
        "                        main_content = content_data.get('main_content', '').strip()\n",
        "                        summary = content_data.get('summary', '').strip()\n",
        "                        key_details = content_data.get('key_details_found', [])\n",
        "\n",
        "                        if main_content and len(main_content) > 100:  # Reduced minimum from 50 to 100\n",
        "                            word_count = len(main_content.split())\n",
        "                            print(f\"            ✅ GPT extracted comprehensive content: {len(main_content)} chars, {word_count} words\")\n",
        "                            if key_details:\n",
        "                                print(f\"            📋 Key details found: {', '.join(key_details[:5])}\")\n",
        "\n",
        "                            return {\n",
        "                                'content': main_content[:5000],  # Increased from 2000 to 5000 chars for analysis\n",
        "                                'summary': summary[:800],        # Increased from 500 to 800 chars\n",
        "                                'status': 'gpt_success',\n",
        "                                'word_count': word_count,\n",
        "                                'key_details': key_details[:10]  # Store key details for tracking\n",
        "                            }\n",
        "\n",
        "                    print(f\"            ⚠️ GPT extraction unsuccessful or content too short\")\n",
        "\n",
        "                except json.JSONDecodeError as je:\n",
        "                    print(f\"            ❌ GPT response JSON parsing failed: {str(je)[:100]}\")\n",
        "\n",
        "            except Exception as gpt_error:\n",
        "                print(f\"            ❌ GPT API error: {gpt_error}\")\n",
        "\n",
        "            # Fallback to title/summary if GPT extraction fails\n",
        "            return {\n",
        "                'content': title,\n",
        "                'summary': title[:300] if title else \"Content extraction failed\",\n",
        "                'status': 'gpt_fallback'\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"            ❌ URL content analysis failed: {e}\")\n",
        "            return {\n",
        "                'content': title,\n",
        "                'summary': title[:300] if title else \"Analysis failed\",\n",
        "                'status': 'error'\n",
        "            }\n",
        "\n",
        "    def _clean_json_response(self, response: str) -> str:\n",
        "        \"\"\"Clean GPT response to extract valid JSON\"\"\"\n",
        "        try:\n",
        "            response = re.sub(r'```json\\s*', '', response)\n",
        "            response = re.sub(r'```\\s*', '', response)\n",
        "\n",
        "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                return json_match.group(0)\n",
        "\n",
        "            return response.strip()\n",
        "        except:\n",
        "            return response\n",
        "\n",
        "# ==========================================\n",
        "# ENHANCED DISASTER ANALYZER WITH LOCALIZATION\n",
        "# ==========================================\n",
        "\n",
        "class EnhancedDisasterAnalyzer:\n",
        "    \"\"\"Complete disaster analyzer with PyGoogleNews localization, GPT-powered intelligence, and client risk scoring\"\"\"\n",
        "\n",
        "    def __init__(self, openai_api_key: str, excel_file_path: str):\n",
        "        self.excel_processor = OptimizedExcelProcessor(excel_file_path)\n",
        "\n",
        "        # Initialize with default news client\n",
        "        self.news_client = LocalizedPyGoogleNewsClient()\n",
        "\n",
        "        # Initialize OpenAI client\n",
        "        try:\n",
        "            if OPENAI_V1:\n",
        "                self.openai_client = OpenAI(api_key=openai_api_key)\n",
        "                self.use_legacy_api = False\n",
        "            else:\n",
        "                openai.api_key = openai_api_key\n",
        "                self.openai_client = None\n",
        "                self.use_legacy_api = True\n",
        "\n",
        "            print(\"✅ OpenAI client initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"❌ OpenAI client initialization failed: {e}\")\n",
        "            raise\n",
        "\n",
        "        # Initialize country-language analyzer (NEW)\n",
        "        self.country_lang_analyzer = CountryLanguageAnalyzer(self.openai_client, self.use_legacy_api)\n",
        "        print(\"✅ Country Language Analyzer initialized - GPT-powered localization active\")\n",
        "\n",
        "        # Initialize GPT URL analyzer\n",
        "        self.url_analyzer = GPTURLAnalyzer(self.openai_client, self.use_legacy_api)\n",
        "        print(\"✅ GPT URL Analyzer initialized\")\n",
        "\n",
        "        # Statistics tracking\n",
        "        self.stats = {\n",
        "            'rows_processed': 0,\n",
        "            'articles_fetched': 0,\n",
        "            'gpt_analyses': 0,\n",
        "            'successful_analyses': 0,\n",
        "            'client_risk_calculations': 0,\n",
        "            'errors': 0,\n",
        "            'api_calls': 0,\n",
        "            'gpt_url_extractions': 0,\n",
        "            'gpt_url_successes': 0,\n",
        "            'country_localizations': 0,\n",
        "            'term_localizations': 0,\n",
        "            'start_time': time.time()\n",
        "        }\n",
        "\n",
        "    def process_single_row(self, row_index: int) -> Optional[Dict]:\n",
        "        \"\"\"Process a single disaster event row with localized search based on client country\"\"\"\n",
        "        try:\n",
        "            gdacs_event = self.excel_processor.get_row_data(row_index)\n",
        "            if not gdacs_event:\n",
        "                print(f\"❌ Failed to load row {row_index}\")\n",
        "                return None\n",
        "\n",
        "            print(f\"\\n🔄 Processing Row {row_index + 1}/{self.excel_processor.get_total_rows()}\")\n",
        "            print(f\"   Event: {gdacs_event.event_name} ({gdacs_event.event_type})\")\n",
        "            print(f\"   Alert: {gdacs_event.alert_level} | Country: {gdacs_event.disaster_country}\")\n",
        "            print(f\"   Client: {gdacs_event.client_country} | Address: {gdacs_event.client_address[:50]}...\")\n",
        "\n",
        "            # Extract date range from Excel data\n",
        "            from_date, to_date = extract_date_range_from_excel_row(gdacs_event)\n",
        "\n",
        "            # Determine language/country for localized search (NEW)\n",
        "            lang_config = self.country_lang_analyzer.analyze_client_country_language(\n",
        "                gdacs_event.client_country, gdacs_event.client_address\n",
        "            )\n",
        "\n",
        "            # Reinitialize news client with localized settings\n",
        "            self.news_client.reinitialize_for_country(lang_config['lang'], lang_config['country'])\n",
        "            self.stats['country_localizations'] += 1\n",
        "\n",
        "            # Search for news articles with localized terms and date filtering (MODIFIED)\n",
        "            articles = self.search_disaster_news_localized(gdacs_event, from_date, to_date, lang_config)\n",
        "\n",
        "            if not articles:\n",
        "                print(\"   ⚠️ No articles found\")\n",
        "                return self._create_empty_result(row_index, gdacs_event)\n",
        "\n",
        "            # Process articles with GPT analysis\n",
        "            print(f\"   📰 Processing {len(articles)} articles with GPT analysis...\")\n",
        "            processed_articles = []\n",
        "            analyses = []\n",
        "\n",
        "            for i, article in enumerate(articles[:20], 1):  # Limit to 20 articles\n",
        "                try:\n",
        "                    print(f\"      Article {i}: {article['title'][:40]}...\")\n",
        "\n",
        "                    # Extract content using GPT URL analyzer\n",
        "                    content_result = self.extract_article_content_gpt(article)\n",
        "                    content = content_result['content']\n",
        "                    article_summary = content_result['summary']\n",
        "\n",
        "                    # Analyze with GPT\n",
        "                    analysis = self.analyze_with_gpt(article['title'], content, gdacs_event)\n",
        "\n",
        "                    # Calculate client risk scores with proper address display (MODIFIED)\n",
        "                    client_risk_scores = self.calculate_client_risk_scores_with_addresses(\n",
        "                        article['title'], content, gdacs_event.event_id)\n",
        "                    analysis.client_risk_scores = client_risk_scores\n",
        "\n",
        "                    if client_risk_scores:\n",
        "                        self.stats['client_risk_calculations'] += 1\n",
        "                        print(f\"         📊 Client Risk Scores: {client_risk_scores}\")\n",
        "\n",
        "                    processed_articles.append({\n",
        "                        'title': article['title'],\n",
        "                        'summary': article_summary,\n",
        "                        'link': article.get('link', ''),\n",
        "                        'source': article.get('source', 'Unknown'),\n",
        "                        'content': content[:500],\n",
        "                        'extraction_method': content_result['status'],\n",
        "                        'word_count': content_result.get('word_count', 0),\n",
        "                        'key_details': content_result.get('key_details', []),\n",
        "                        'lang_country': article.get('lang_country', f\"{lang_config['lang']}-{lang_config['country']}\")  # NEW\n",
        "                    })\n",
        "\n",
        "                    analyses.append(analysis)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"         ❌ Error processing article: {e}\")\n",
        "                    continue\n",
        "\n",
        "            # Calculate summary\n",
        "            summary = self._calculate_summary(analyses)\n",
        "            print(f\"   ✅ Completed: {len(analyses)} articles analyzed with localized search\")\n",
        "            print(f\"   🌍 Language/Country: {lang_config['lang']}-{lang_config['country']}\")\n",
        "\n",
        "            result = {\n",
        "                'row_index': row_index,\n",
        "                'gdacs_event': gdacs_event,\n",
        "                'articles': processed_articles,\n",
        "                'analyses': analyses,\n",
        "                'summary': summary,\n",
        "                'date_range': {'from_date': from_date, 'to_date': to_date},\n",
        "                'localization': lang_config  # NEW: Track localization used\n",
        "            }\n",
        "\n",
        "            self.stats['rows_processed'] += 1\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   ❌ Row processing failed: {e}\")\n",
        "            self.stats['errors'] += 1\n",
        "            return None\n",
        "\n",
        "    def search_disaster_news_localized(self, gdacs_event: GDACSEvent, from_date: str = None, to_date: str = None, lang_config: Dict = None) -> List[Dict]:\n",
        "        \"\"\"Search for relevant disaster news with localized terms and PyGoogleNews\"\"\"\n",
        "\n",
        "        if not lang_config:\n",
        "            lang_config = {'lang': 'en', 'country': 'US'}\n",
        "\n",
        "        # Create basic English search terms\n",
        "        english_terms = []\n",
        "\n",
        "        # Create targeted search terms\n",
        "        if gdacs_event.disaster_country and gdacs_event.disaster_country.strip():\n",
        "            country_terms = gdacs_event.disaster_country.split(',')[:2]  # Limit countries\n",
        "            for country in country_terms:\n",
        "                country = country.strip()\n",
        "                if country and len(country) > 1:\n",
        "                    english_terms.extend([\n",
        "                        f\"{gdacs_event.event_type} {country}\",\n",
        "                        f\"{country} {gdacs_event.event_type}\"\n",
        "                    ])\n",
        "\n",
        "        # Add event name if available\n",
        "        if gdacs_event.event_name and len(gdacs_event.event_name.strip()) > 5:\n",
        "            english_terms.append(gdacs_event.event_name)\n",
        "\n",
        "        # Add generic disaster type terms\n",
        "        english_terms.extend([\n",
        "            gdacs_event.event_type,\n",
        "            f\"{gdacs_event.event_type} disaster\",\n",
        "            f\"{gdacs_event.event_type} emergency\"\n",
        "        ])\n",
        "\n",
        "        # Localize terms using GPT (NEW)\n",
        "        if lang_config['lang'] != 'en':\n",
        "            print(f\"   🌐 Localizing search terms to {lang_config['lang']} for {lang_config['country']}\")\n",
        "            localized_terms = self.country_lang_analyzer.localize_disaster_terms(\n",
        "                english_terms[:5], lang_config['lang'], lang_config['country']\n",
        "            )\n",
        "            self.stats['term_localizations'] += 1\n",
        "            search_terms = localized_terms[:6]  # Use localized terms\n",
        "        else:\n",
        "            search_terms = english_terms[:6]  # Use English terms\n",
        "\n",
        "        all_articles = []\n",
        "        for term in search_terms:\n",
        "            try:\n",
        "                # Use PyGoogleNews with date filtering\n",
        "                results = self.news_client.search(\n",
        "                    query=term,\n",
        "                    max_results=15,\n",
        "                    from_=from_date,\n",
        "                    to_=to_date\n",
        "                )\n",
        "\n",
        "                if results['status'] == 'ok':\n",
        "                    # Add lang_country info to each article\n",
        "                    for article in results['entries']:\n",
        "                        article['lang_country'] = results.get('lang_country', f\"{lang_config['lang']}-{lang_config['country']}\")\n",
        "                    all_articles.extend(results['entries'])\n",
        "\n",
        "                time.sleep(1)  # Rate limiting\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"         Search error for '{term}': {e}\")\n",
        "                continue\n",
        "\n",
        "        # Remove duplicates and limit results\n",
        "        unique_articles = self._remove_duplicates(all_articles)\n",
        "        final_articles = unique_articles[:20]\n",
        "\n",
        "        self.stats['articles_fetched'] += len(final_articles)\n",
        "        return final_articles\n",
        "\n",
        "    def extract_article_content_gpt(self, article: Dict) -> Dict[str, str]:\n",
        "        \"\"\"Extract content from article URL using GPT analysis\"\"\"\n",
        "        url = article.get('link', '')\n",
        "        title = article.get('title', '')\n",
        "\n",
        "        self.stats['gpt_url_extractions'] += 1\n",
        "\n",
        "        # Use GPT URL analyzer\n",
        "        content_result = self.url_analyzer.extract_url_content_with_gpt(url, title)\n",
        "\n",
        "        if content_result['status'] == 'gpt_success':\n",
        "            self.stats['gpt_url_successes'] += 1\n",
        "\n",
        "        return content_result\n",
        "\n",
        "    def analyze_with_gpt(self, title: str, content: str, gdacs_event: GDACSEvent) -> DisasterAnalysis:\n",
        "        \"\"\"Analyze article with GPT using enhanced prompt\"\"\"\n",
        "        try:\n",
        "            disaster_info = GDACS_SCORING_KNOWLEDGE.get(gdacs_event.event_type, {})\n",
        "            alert_info = ALERT_LEVEL_DEFINITIONS.get(gdacs_event.alert_level, {})\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "You are a GDACS disaster expert analyzing media coverage of a specific GDACS event with deep understanding of GDACS scoring methodology.\n",
        "\n",
        "GDACS EVENT CONTEXT:\n",
        "- Event ID: {gdacs_event.event_id}\n",
        "- Event Type: {gdacs_event.event_type} ({disaster_info.get('name', 'Unknown')})\n",
        "- Event Name: {gdacs_event.event_name}\n",
        "- GDACS Alert Level: {gdacs_event.alert_level} ({alert_info.get('severity', 'Unknown')} Severity)\n",
        "- GDACS Alert Score: {gdacs_event.alert_score}\n",
        "- GDACS Countries: {gdacs_event.disaster_country}\n",
        "- GDACS Description: {gdacs_event.event_description}\n",
        "- Severity Value: {gdacs_event.severity_value}\n",
        "- Severity Text: {gdacs_event.severity_text}\n",
        "- Date Range: {gdacs_event.from_date} to {gdacs_event.to_date}\n",
        "\n",
        "GDACS {gdacs_event.event_type} SCORING METHODOLOGY:\n",
        "- Disaster Name: {disaster_info.get('name', 'Unknown')}\n",
        "- GDACS Models Used: {', '.join(disaster_info.get('models', []))}\n",
        "- Key Assessment Inputs: {', '.join(disaster_info.get('key_inputs', []))}\n",
        "- GDACS Scoring Method: {disaster_info.get('scoring_method', 'Standard methodology')}\n",
        "- GDACS Alert Thresholds: {disaster_info.get('alert_thresholds', {})}\n",
        "- Key Assessment Factors: {', '.join(disaster_info.get('key_factors', []))}\n",
        "- Expected Impact Indicators: {', '.join(disaster_info.get('impact_indicators', []))}\n",
        "- Assessment Focus: {disaster_info.get('assessment_focus', 'Standard assessment')}\n",
        "\n",
        "ALERT LEVEL MEANING:\n",
        "- {gdacs_event.alert_level} Alert Definition: {alert_info.get('definition', 'No definition')}\n",
        "- Description: {alert_info.get('description', 'No description')}\n",
        "- Required Action: {alert_info.get('action_required', 'Unknown')}\n",
        "\n",
        "MEDIA ARTICLE TO ANALYZE:\n",
        "Title: {title}\n",
        "Content: {content[:3000]}  {f\"... (Content truncated from {len(content)} characters for analysis)\" if len(content) > 3000 else \"\"}\n",
        "\n",
        "Please analyze this media article about the GDACS {gdacs_event.event_type} event considering the GDACS context and scoring methodology. Return ONLY a JSON object:\n",
        "\n",
        "{{\n",
        "    \"disaster_severity_score\": <0-100>,\n",
        "    \"response_capability_score\": <0-100>,\n",
        "    \"response_timeliness_score\": <0-100>,\n",
        "    \"disaster_relevance_score\": <0-100>,\n",
        "    \"gdacs_alignment_score\": <0-100>,\n",
        "    \"media_coverage_score\": <0-100>,\n",
        "    \"is_disaster_related\": <true/false>,\n",
        "    \"primary_disaster_type\": \"<type>\",\n",
        "    \"disaster_keywords\": [\"keyword1\", \"keyword2\", ...],\n",
        "    \"response_keywords\": [\"specific response measure 1\", \"specific response measure 2\", ...],\n",
        "    \"timeliness_keywords\": [\"specific timeframe 1\", \"specific timeframe 2\", ...],\n",
        "    \"analysis_summary\": \"<150 words>\",\n",
        "    \"gdacs_context_analysis\": \"<100 words>\",\n",
        "    \"final_risk_score\": <0-100>,\n",
        "    \"confidence_score\": <0-100>\n",
        "}}\n",
        "\n",
        "ENHANCED SCORING GUIDELINES (with GDACS context):\n",
        "\n",
        "Do not return any zeros. Analyze all articles and list the scoring content relevant to each category.\n",
        "\n",
        "1. disaster_severity_score (0-100):\n",
        "   - Use GDACS alert score {gdacs_event.alert_score} as authoritative baseline\n",
        "   - Consider {gdacs_event.event_type} specific factors: {', '.join(disaster_info.get('key_factors', []))}\n",
        "   - Look for impact indicators: {', '.join(disaster_info.get('impact_indicators', []))}\n",
        "   - Expected for {gdacs_event.alert_level}: {alert_info.get('description', 'standard impact')}\n",
        "   - GDACS Assessment Focus: {disaster_info.get('assessment_focus', 'standard assessment')}\n",
        "\n",
        "2. response_capability_score (0-100) - ENHANCED RESILIENCE ASSESSMENT:\n",
        "   - INFRASTRUCTURE RESILIENCE: Are critical infrastructure systems (hospitals, power, water, communication) functioning or quickly restored?\n",
        "   - INSTITUTIONAL CAPACITY: Do local/regional authorities demonstrate effective crisis management? Are emergency services adequately staffed and equipped?\n",
        "   - COMMUNITY PREPAREDNESS: Evidence of evacuation plans, early warning systems, disaster drills, or community resilience programs\n",
        "   - RESOURCE AVAILABILITY: Sufficient emergency supplies, medical resources, shelter capacity, search and rescue capabilities\n",
        "   - COORDINATION EFFECTIVENESS: Multi-agency cooperation, clear command structure, information sharing between organizations\n",
        "   - ADAPTATION MEASURES: Evidence of previous disaster experience improving current response, building codes, land use planning\n",
        "   - ECONOMIC RESILIENCE: Local economy's ability to maintain essential services, business continuity plans, rapid recovery capacity\n",
        "   - SOCIAL COHESION: Community networks, volunteer organizations, social support systems helping with disaster response\n",
        "   - VULNERABILITY REDUCTION: Pre-disaster risk reduction measures, protective infrastructure, public education programs\n",
        "   - Look for specific evidence of resilience factors rather than general statements about government response\n",
        "\n",
        "3. response_timeliness_score (0-100):\n",
        "   - Speed of response relative to {gdacs_event.event_type} onset characteristics\n",
        "   - Warning system effectiveness and advance notice provided\n",
        "   - Proactive vs reactive measures and preparedness activation timing\n",
        "\n",
        "4. disaster_relevance_score (0-100):\n",
        "   - How much this article focuses on actual disaster impacts vs other topics\n",
        "   - Operational vs academic content\n",
        "   - IMPORTANT: Score should be VERY LOW (0-20) if article is about business disruptions, supply chain issues, economic impacts, or general news unrelated to actual natural disasters or emergency events\n",
        "   - HIGH scores (70-100) only for articles directly about natural disasters, emergency responses, casualties, damage, evacuations, or rescue operations\n",
        "\n",
        "5. gdacs_alignment_score (0-100):\n",
        "   - How well the media report aligns with GDACS {gdacs_event.alert_level} assessment\n",
        "   - Consistency with GDACS scoring methodology for {gdacs_event.event_type}\n",
        "   - Agreement with expected {gdacs_event.event_type} impact patterns\n",
        "\n",
        "6. media_coverage_score (0-100):\n",
        "   - Quality and depth of media coverage\n",
        "   - Accuracy of disaster information reported\n",
        "   - Completeness of impact assessment\n",
        "\n",
        "7. final_risk_score (0-100):\n",
        "   - Calculate a weighted composite score using the following methodology:\n",
        "   - Primary Weight (60%): disaster_severity_score (most critical for risk assessment - the inherent severity of the disaster itself)\n",
        "   - Secondary Weight (30%): response_capability_score (significantly affects actual risk to populations - government/institutional response capability)\n",
        "   - Quality Weight (10%): media_coverage_score (reliability and completeness of information source affects assessment accuracy)\n",
        "   - Formula: (disaster_severity × 0.6) + (response_capability × 0.3) + (media_coverage × 0.1)\n",
        "   - Consider GDACS alert level as validation: RED events should generally score 70-100, ORANGE 40-80, GREEN 10-50\n",
        "   - Higher disaster severity with poor response capability should yield higher risk scores\n",
        "   - Strong response capability can moderately reduce overall risk even for severe disasters\n",
        "\n",
        "8. confidence_score (0-100):\n",
        "\n",
        "Calculate the confidence_score as a weighted composite of the following factors:\n",
        "\n",
        "confidence_score = int(\n",
        "  Formula: 0.3 * disaster_severity_score + 0.3 * response_capability_score + 0.2 * media_coverage_score + 0.2 * gdacs_alignment_score\n",
        ")\n",
        "\n",
        "Adjustment rules:\n",
        "- If disaster_relevance_score < 30, subtract 12 points (penalty for weak disaster relevance).\n",
        "- If media_coverage_score < 40, subtract 8 points (penalty for poor information quality).\n",
        "- If gdacs_alignment_score < 50, subtract 5 points (penalty for inconsistency with GDACS).\n",
        "- Final score must be clamped between 1 and 100 (no zeros).\n",
        "\n",
        "Confidence levels:\n",
        "- 80–100 → High confidence\n",
        "- 50–79 → Medium confidence\n",
        "- 20–49 → Low confidence\n",
        "- 1–19 → Very Low confidence\n",
        "\n",
        "Key idea: confidence_score reflects how reliable the article analysis is, based on clarity of disaster impact, completeness of response information, quality of media coverage, disaster relevance, and alignment with GDACS classification.\n",
        "\n",
        "CRITICAL DISASTER RELEVANCE CHECK:\n",
        "Before assigning is_disaster_related as true, verify that the article is ACTUALLY about:\n",
        "- Natural disasters (earthquakes, floods, typhoons, etc.)\n",
        "- Emergency situations requiring humanitarian response\n",
        "- Rescue and evacuation operations\n",
        "- Casualty reports and damage assessments\n",
        "- Government disaster response activities\n",
        "- Relief and recovery efforts\n",
        "\n",
        "DO NOT classify as disaster-related if the article is primarily about:\n",
        "- Business disruptions or economic impacts without actual disaster focus\n",
        "- Supply chain issues without disaster context\n",
        "- General business news or market reports\n",
        "- Political news unrelated to disaster response\n",
        "- Technology or infrastructure issues without disaster context\n",
        "- General social issues not related to specific disasters\n",
        "\n",
        "Set is_disaster_related to FALSE if the article is not genuinely focused on disaster events or emergency response.\n",
        "\n",
        "SPECIAL INSTRUCTIONS FOR KEYWORDS:\n",
        "\n",
        "For response_keywords - Extract SPECIFIC response measures mentioned in the article, such as:\n",
        "- \"deployed 500 rescue personnel to disaster area\"\n",
        "- \"opened 50 emergency shelters\"\n",
        "- \"activated national emergency medical system\"\n",
        "- \"deployed military helicopters for rescue operations\"\n",
        "- \"established temporary water supply stations\"\n",
        "- \"distributed emergency food supplies to 10,000 families\"\n",
        "- \"set up field hospitals with 200 beds\"\n",
        "Instead of generic terms like \"rescue\", \"evacuation\", provide the actual detailed measures described.\n",
        "\n",
        "For timeliness_keywords - Extract SPECIFIC timeframes and deadlines mentioned:\n",
        "- \"evacuation completed within 24 hours\"\n",
        "- \"power expected to be restored within 3 days\"\n",
        "- \"rescue operations started 2 hours after disaster\"\n",
        "- \"next 48 hours critical for rescue efforts\"\n",
        "- \"temporary housing rebuilt within one week\"\n",
        "- \"emergency supplies delivered within 6 hours\"\n",
        "- \"assessment teams deployed within 12 hours\"\n",
        "Instead of generic terms like \"immediate\", \"soon\", \"quickly\", provide actual timeframes mentioned.\n",
        "\n",
        "For gdacs_context_analysis: Analyze how well this media coverage reflects the GDACS {gdacs_event.alert_level} classification and {gdacs_event.event_type} scoring methodology.\n",
        "\n",
        "This media article is about the GDACS {gdacs_event.event_type} event with {gdacs_event.alert_level} alert level (score: {gdacs_event.alert_score}).\n",
        "\n",
        "It’s possible that {gdacs_event.disaster_country} and {gdacs_event.client_country} are not the same, so we must evaluate both the geographic distance between the location and the client country as well as the news content to determine the final score.\n",
        "\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\n",
        "                    \"role\": \"system\",\n",
        "                    \"content\": \"You are a GDACS-trained disaster expert who understands GDACS scoring methodologies and analyzes media coverage with this context. Always respond with valid JSON.\"\n",
        "                },\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            # Make API call\n",
        "            try:\n",
        "                self.stats['api_calls'] += 1\n",
        "\n",
        "                if self.use_legacy_api:\n",
        "                    response = openai.ChatCompletion.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=messages,\n",
        "                        max_tokens=1500,\n",
        "                        temperature=0.2\n",
        "                    )\n",
        "                    result = response.choices[0].message.content.strip()\n",
        "                else:\n",
        "                    response = self.openai_client.chat.completions.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=messages,\n",
        "                        max_tokens=1500,\n",
        "                        temperature=0.2\n",
        "                    )\n",
        "                    result = response.choices[0].message.content.strip()\n",
        "\n",
        "                # Clean and parse JSON response\n",
        "                result = self._clean_json_response(result)\n",
        "\n",
        "                try:\n",
        "                    analysis_data = json.loads(result)\n",
        "                except json.JSONDecodeError:\n",
        "                    return self._create_fallback_analysis(gdacs_event)\n",
        "\n",
        "                # Create analysis object with validation\n",
        "                analysis = DisasterAnalysis()\n",
        "\n",
        "                analysis.disaster_severity_score = self._validate_score(analysis_data.get('disaster_severity_score', 50))\n",
        "                analysis.response_capability_score = self._validate_score(analysis_data.get('response_capability_score', 40))\n",
        "                analysis.response_timeliness_score = self._validate_score(analysis_data.get('response_timeliness_score', 40))\n",
        "                analysis.disaster_relevance_score = self._validate_score(analysis_data.get('disaster_relevance_score', 60))\n",
        "                analysis.gdacs_alignment_score = self._validate_score(analysis_data.get('gdacs_alignment_score', 50))\n",
        "                analysis.media_coverage_score = self._validate_score(analysis_data.get('media_coverage_score', 50))\n",
        "                analysis.final_risk_score = self._validate_score(analysis_data.get('final_risk_score', 50))\n",
        "                analysis.confidence_score = self._validate_score(analysis_data.get('confidence_score', 60))\n",
        "\n",
        "                analysis.is_disaster_related = bool(analysis_data.get('is_disaster_related', True))\n",
        "                analysis.primary_disaster_type = str(analysis_data.get('primary_disaster_type', gdacs_event.event_type))\n",
        "                analysis.gpt_analysis_summary = str(analysis_data.get('analysis_summary', ''))[:300]\n",
        "                analysis.gdacs_context_analysis = str(analysis_data.get('gdacs_context_analysis', ''))[:200]\n",
        "\n",
        "                analysis.gpt_disaster_keywords = self._validate_keywords(analysis_data.get('disaster_keywords', []))\n",
        "                analysis.gpt_response_keywords = self._validate_keywords(analysis_data.get('response_keywords', []))\n",
        "                analysis.gpt_timeliness_keywords = self._validate_keywords(analysis_data.get('timeliness_keywords', []))\n",
        "\n",
        "                # Set classification levels\n",
        "                analysis.severity_level = self._get_severity_level(analysis.disaster_severity_score)\n",
        "                analysis.response_level = self._get_response_level(analysis.response_capability_score)\n",
        "                analysis.timeliness_level = self._get_timeliness_level(analysis.response_timeliness_score)\n",
        "                analysis.relevance_level = self._get_relevance_level(analysis.disaster_relevance_score)\n",
        "                analysis.gdacs_alignment_level = self._get_gdacs_alignment_level(analysis.gdacs_alignment_score)\n",
        "                analysis.media_coverage_level = self._get_media_coverage_level(analysis.media_coverage_score)\n",
        "\n",
        "                # Check client country match\n",
        "                analysis.client_country_match = self._check_client_country_match(gdacs_event, content)\n",
        "\n",
        "                self.stats['gpt_analyses'] += 1\n",
        "                self.stats['successful_analyses'] += 1\n",
        "                return analysis\n",
        "\n",
        "            except Exception as api_error:\n",
        "                print(f\"            API error: {api_error}\")\n",
        "                return self._create_fallback_analysis(gdacs_event)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"            Analysis failed: {e}\")\n",
        "            self.stats['errors'] += 1\n",
        "            return self._create_fallback_analysis(gdacs_event)\n",
        "\n",
        "    def calculate_client_risk_scores_with_addresses(self, title: str, content: str, event_id: str) -> Dict[str, float]:\n",
        "        \"\"\"Calculate risk scores for clients using actual Client_Address names instead of codes\"\"\"\n",
        "        clients = self.excel_processor.get_client_info_for_event(event_id)\n",
        "\n",
        "        if not clients:\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            # Prepare client information for GPT with actual addresses\n",
        "            client_details = []\n",
        "            for client in clients:\n",
        "                client_details.append({\n",
        "                    'client_address': client.client_address,  # Use full address as identifier\n",
        "                    'client_country': client.client_country,\n",
        "                    'severity_text': client.severity_text,\n",
        "                    'distance_summary': client.client_distance_summary\n",
        "                })\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "You are a disaster risk assessment expert. Please analyze this news article and calculate risk scores (1-10) for each client based on their geographical location and the disaster impact.\n",
        "\n",
        "Article Title: {title}\n",
        "Article Content: {content[:4000]}  {f\"... (Content truncated from {len(content)} characters for analysis)\" if len(content) > 4000 else \"\"}\n",
        "\n",
        "Client Information:\n",
        "{json.dumps(client_details, indent=2)}\n",
        "\n",
        "Instructions:\n",
        "1. Analyze the disaster severity and impact described in the news article\n",
        "2. For each client, use their Client_Address as the primary identifier\n",
        "3. For each client, FIRST determine the exact geographical location of their Client_Address:\n",
        "   - Identify the specific city, district, province/state, or region where the client is located\n",
        "   - Consider the detailed geographical context of the address\n",
        "   - Use your geographical knowledge to understand the precise location relative to the disaster center\n",
        "4. For each client, consider:\n",
        "   - Severity_Text: How severe is the impact in their specific area/region?\n",
        "   - Client_Distance_Summary: How close are they to the disaster center?\n",
        "   - The detailed geographical location derived from Client_Address\n",
        "   - The overall disaster impact described in the news relative to their specific location\n",
        "5. Assign risk scores (1-10):\n",
        "   - 1-2: Minimal risk (very far from disaster center, minimal regional impact)\n",
        "   - 3-4: Low risk (moderate distance, light regional impact)\n",
        "   - 5-6: Medium risk (closer regional proximity, moderate regional impact)\n",
        "   - 7-8: High risk (close regional proximity, significant regional impact)\n",
        "   - 9-10: Critical risk (very close to disaster center, severe regional impact)\n",
        "\n",
        "Return ONLY a JSON object with this exact format:\n",
        "{{\n",
        "    \"client_risk_scores\": {{\n",
        "        \"Full_Client_Address_1\": risk_score,\n",
        "        \"Full_Client_Address_2\": risk_score,\n",
        "        ...\n",
        "    }},\n",
        "    \"detailed_location_analysis\": {{\n",
        "        \"Full_Client_Address_1\": \"Detailed geographical location analysis and risk assessment reasoning\",\n",
        "        \"Full_Client_Address_2\": \"Detailed geographical location analysis and risk assessment reasoning\",\n",
        "        ...\n",
        "    }},\n",
        "    \"risk_assessment_summary\": \"Brief explanation of overall risk assessment logic based on detailed locations\"\n",
        "}}\n",
        "\n",
        "IMPORTANT:\n",
        "- Use the FULL Client_Address as the key in client_risk_scores (not country codes like MX, CN, etc.)\n",
        "- Base your risk assessment on the DETAILED GEOGRAPHICAL LOCATIONS derived from Client_Address\n",
        "- Provide clear reasoning for each location's risk assessment\n",
        "\"\"\"\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": \"You are a geographical risk assessment expert. Always respond with valid JSON only, no markdown formatting.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt}\n",
        "            ]\n",
        "\n",
        "            try:\n",
        "                self.stats['api_calls'] += 1\n",
        "\n",
        "                if self.use_legacy_api:\n",
        "                    response = openai.ChatCompletion.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=messages,\n",
        "                        max_tokens=800,\n",
        "                        temperature=0.1\n",
        "                    )\n",
        "                    result = response.choices[0].message.content.strip()\n",
        "                else:\n",
        "                    response = self.openai_client.chat.completions.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=messages,\n",
        "                        max_tokens=800,\n",
        "                        temperature=0.1\n",
        "                    )\n",
        "                    result = response.choices[0].message.content.strip()\n",
        "\n",
        "                # Clean and parse JSON response\n",
        "                cleaned_result = self._clean_json_response(result)\n",
        "\n",
        "                try:\n",
        "                    risk_data = json.loads(cleaned_result)\n",
        "                    risk_scores = risk_data.get('client_risk_scores', {})\n",
        "                    location_analysis = risk_data.get('detailed_location_analysis', {})\n",
        "\n",
        "                    # Ensure all scores are floats between 1-10\n",
        "                    for address, score in risk_scores.items():\n",
        "                        try:\n",
        "                            risk_scores[address] = max(1.0, min(10.0, float(score)))\n",
        "                        except (ValueError, TypeError):\n",
        "                            risk_scores[address] = 5.0  # Default medium risk\n",
        "\n",
        "                    logger.info(f\"✅ Calculated risk scores for {len(risk_scores)} clients using addresses\")\n",
        "\n",
        "                    # Log detailed location analysis\n",
        "                    if location_analysis:\n",
        "                        logger.info(\"📍 Detailed Location Analysis:\")\n",
        "                        for address, analysis in location_analysis.items():\n",
        "                            logger.info(f\"   {address[:50]}...: {analysis}\")\n",
        "\n",
        "                    return risk_scores\n",
        "\n",
        "                except json.JSONDecodeError as e:\n",
        "                    logger.warning(f\"JSON parsing failed: {e}\")\n",
        "                    # Try to extract just the client_risk_scores part as fallback\n",
        "                    try:\n",
        "                        scores_match = re.search(r'\"client_risk_scores\":\\s*(\\{[^}]+\\})', cleaned_result)\n",
        "                        if scores_match:\n",
        "                            scores_json = scores_match.group(1)\n",
        "                            risk_scores = json.loads(scores_json)\n",
        "                            # Ensure all scores are floats between 1-10\n",
        "                            for address, score in risk_scores.items():\n",
        "                                try:\n",
        "                                    risk_scores[address] = max(1.0, min(10.0, float(score)))\n",
        "                                except (ValueError, TypeError):\n",
        "                                    risk_scores[address] = 5.0\n",
        "                            logger.info(f\"✅ Fallback: Extracted {len(risk_scores)} client risk scores\")\n",
        "                            return risk_scores\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    logger.warning(f\"Complete JSON parsing failure. Raw response: {cleaned_result[:200]}...\")\n",
        "                    return {}\n",
        "\n",
        "            except Exception as api_error:\n",
        "                logger.error(f\"❌ Client risk API call failed: {api_error}\")\n",
        "                return {}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"❌ Client risk score calculation failed: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def process_batch_rows(self, start_row: int, num_rows: int) -> List[Dict]:\n",
        "        \"\"\"Process a batch of disaster event rows with localization\"\"\"\n",
        "        total_rows = self.excel_processor.get_total_rows()\n",
        "\n",
        "        if start_row >= total_rows:\n",
        "            print(f\"Start row {start_row} exceeds total rows {total_rows}\")\n",
        "            return []\n",
        "\n",
        "        end_row = min(start_row + num_rows, total_rows)\n",
        "        print(f\"\\n📊 Processing batch: rows {start_row} to {end_row - 1} ({end_row - start_row} rows)\")\n",
        "        print(f\"🌍 Using PyGoogleNews with GPT-powered localization\")\n",
        "\n",
        "        batch_results = []\n",
        "        start_time = time.time()\n",
        "\n",
        "        for row_index in range(start_row, end_row):\n",
        "            try:\n",
        "                result = self.process_single_row(row_index)\n",
        "                if result:\n",
        "                    batch_results.append(result)\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(f\"\\n⚠️ Batch processing interrupted at row {row_index}\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing row {row_index}: {e}\")\n",
        "                continue\n",
        "\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"\\n✅ Batch completed: {len(batch_results)} rows processed in {elapsed_time/60:.1f} minutes\")\n",
        "        print(f\"🌍 Localization Stats: {self.stats['country_localizations']} countries, {self.stats['term_localizations']} term sets\")\n",
        "\n",
        "        return batch_results\n",
        "\n",
        "    # Helper methods (mostly unchanged, keeping all existing helper methods)\n",
        "    def _remove_duplicates(self, articles: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"Remove duplicate articles based on title similarity\"\"\"\n",
        "        if len(articles) <= 1:\n",
        "            return articles\n",
        "\n",
        "        unique_articles = []\n",
        "        seen_titles = set()\n",
        "\n",
        "        for article in articles:\n",
        "            title = article.get('title', '').lower().strip()\n",
        "\n",
        "            if not title or len(title) < 10:\n",
        "                continue\n",
        "\n",
        "            normalized = re.sub(r'[^\\w\\s]', '', title)\n",
        "            normalized = re.sub(r'\\s+', ' ', normalized).strip()\n",
        "\n",
        "            if normalized not in seen_titles:\n",
        "                unique_articles.append(article)\n",
        "                seen_titles.add(normalized)\n",
        "\n",
        "                if len(unique_articles) >= 50:\n",
        "                    break\n",
        "\n",
        "        return unique_articles\n",
        "\n",
        "    def _clean_json_response(self, response: str) -> str:\n",
        "        \"\"\"Clean GPT response to extract valid JSON\"\"\"\n",
        "        try:\n",
        "            response = re.sub(r'```json\\s*', '', response)\n",
        "            response = re.sub(r'```\\s*', '', response)\n",
        "\n",
        "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                return json_match.group(0)\n",
        "\n",
        "            return response.strip()\n",
        "        except:\n",
        "            return response\n",
        "\n",
        "    def _validate_score(self, score) -> float:\n",
        "        \"\"\"Validate and normalize score to 0-100 range\"\"\"\n",
        "        try:\n",
        "            score = float(score)\n",
        "            return max(0.0, min(100.0, score))\n",
        "        except (ValueError, TypeError):\n",
        "            return 50.0\n",
        "\n",
        "    def _validate_keywords(self, keywords) -> List[str]:\n",
        "        \"\"\"Validate and clean keywords list\"\"\"\n",
        "        if not isinstance(keywords, list):\n",
        "            return []\n",
        "\n",
        "        valid_keywords = []\n",
        "        for keyword in keywords:\n",
        "            if isinstance(keyword, str) and len(keyword.strip()) > 0:\n",
        "                valid_keywords.append(str(keyword).strip()[:100])\n",
        "\n",
        "        return valid_keywords[:5]\n",
        "\n",
        "    def _get_severity_level(self, score: float) -> str:\n",
        "        if score >= 80: return \"Critical\"\n",
        "        elif score >= 60: return \"High\"\n",
        "        elif score >= 40: return \"Medium\"\n",
        "        elif score >= 20: return \"Low\"\n",
        "        else: return \"Minimal\"\n",
        "\n",
        "    def _get_response_level(self, score: float) -> str:\n",
        "        if score >= 80: return \"Excellent\"\n",
        "        elif score >= 60: return \"Good\"\n",
        "        elif score >= 40: return \"Adequate\"\n",
        "        elif score >= 20: return \"Limited\"\n",
        "        else: return \"Poor\"\n",
        "\n",
        "    def _get_timeliness_level(self, score: float) -> str:\n",
        "        if score >= 80: return \"Immediate\"\n",
        "        elif score >= 60: return \"Rapid\"\n",
        "        elif score >= 40: return \"Timely\"\n",
        "        elif score >= 20: return \"Delayed\"\n",
        "        else: return \"Slow\"\n",
        "\n",
        "    def _get_relevance_level(self, score: float) -> str:\n",
        "        if score >= 80: return \"Highly Relevant\"\n",
        "        elif score >= 60: return \"Relevant\"\n",
        "        elif score >= 40: return \"Moderately Relevant\"\n",
        "        elif score >= 20: return \"Somewhat Relevant\"\n",
        "        else: return \"Not Relevant\"\n",
        "\n",
        "    def _get_gdacs_alignment_level(self, score: float) -> str:\n",
        "        if score >= 80: return \"Highly Aligned\"\n",
        "        elif score >= 60: return \"Well Aligned\"\n",
        "        elif score >= 40: return \"Moderately Aligned\"\n",
        "        elif score >= 20: return \"Poorly Aligned\"\n",
        "        else: return \"Not Aligned\"\n",
        "\n",
        "    def _get_media_coverage_level(self, score: float) -> str:\n",
        "        if score >= 80: return \"Excellent Coverage\"\n",
        "        elif score >= 60: return \"Good Coverage\"\n",
        "        elif score >= 40: return \"Adequate Coverage\"\n",
        "        elif score >= 20: return \"Limited Coverage\"\n",
        "        else: return \"Poor Coverage\"\n",
        "\n",
        "    def _expand_client_country_names(self, client_countries: List[str]) -> List[str]:\n",
        "        \"\"\"Convert client country codes/addresses to full country names based on actual client warehouse locations\"\"\"\n",
        "        # Country code mappings based on actual client warehouse data\n",
        "        country_mappings = {\n",
        "            'MX': ['mexico', 'mexican'],\n",
        "            'CN': ['china', 'prc', 'chinese'],\n",
        "            'HK': ['hong kong', 'hongkong'],\n",
        "            'SG': ['singapore'],\n",
        "            'IN': ['india', 'indian'],\n",
        "            'US': ['united states', 'usa', 'america', 'american'],\n",
        "            'MY': ['malaysia', 'malaysian'],\n",
        "            'NL': ['netherlands', 'holland', 'dutch'],\n",
        "            'RO': ['romania', 'romanian'],\n",
        "            'HU': ['hungary', 'hungarian']\n",
        "        }\n",
        "\n",
        "        expanded_names = []\n",
        "\n",
        "        for client in client_countries:\n",
        "            client_clean = client.strip()\n",
        "\n",
        "            # If it's a known country code from our client data, add the full names\n",
        "            if client_clean.upper() in country_mappings:\n",
        "                expanded_names.extend(country_mappings[client_clean.upper()])\n",
        "                expanded_names.append(client_clean.lower())  # Also keep original\n",
        "            else:\n",
        "                # For longer addresses, extract potential country names\n",
        "                client_lower = client_clean.lower()\n",
        "\n",
        "                # Specific patterns based on actual client addresses\n",
        "                if any(term in client_lower for term in ['mexico', 'mexican', 'carretera', 'lopez mateos']):\n",
        "                    expanded_names.extend(['mexico', 'mexican'])\n",
        "                elif any(term in client_lower for term in ['china', 'chinese', 'suhong road', 'guan pu rd', 'suqian rd']):\n",
        "                    expanded_names.extend(['china', 'prc', 'chinese'])\n",
        "                elif any(term in client_lower for term in ['hong kong', 'hongkong', 'kam pok road', 'yeung uk road', 'san tin']):\n",
        "                    expanded_names.extend(['hong kong', 'hongkong'])\n",
        "                elif any(term in client_lower for term in ['singapore', 'alps avenue']):\n",
        "                    expanded_names.extend(['singapore'])\n",
        "                elif any(term in client_lower for term in ['india', 'indian', 'sipcot']):\n",
        "                    expanded_names.extend(['india', 'indian'])\n",
        "                elif any(term in client_lower for term in ['united states', 'america', 'usa', 'yosemite dr']):\n",
        "                    expanded_names.extend(['united states', 'usa', 'america', 'american'])\n",
        "                elif any(term in client_lower for term in ['malaysia', 'malaysian', 'mukim']):\n",
        "                    expanded_names.extend(['malaysia', 'malaysian'])\n",
        "                elif any(term in client_lower for term in ['netherlands', 'holland', 'dutch']):\n",
        "                    expanded_names.extend(['netherlands', 'holland', 'dutch'])\n",
        "                elif any(term in client_lower for term in ['romania', 'romanian']):\n",
        "                    expanded_names.extend(['romania', 'romanian'])\n",
        "                elif any(term in client_lower for term in ['hungary', 'hungarian']):\n",
        "                    expanded_names.extend(['hungary', 'hungarian'])\n",
        "                else:\n",
        "                    # Add the original as is for other cases\n",
        "                    expanded_names.append(client_lower)\n",
        "\n",
        "        return list(set(expanded_names))  # Remove duplicates\n",
        "\n",
        "    def _check_client_country_match(self, gdacs_event: GDACSEvent, content: str) -> bool:\n",
        "        \"\"\"Check if content mentions client country using expanded country names\"\"\"\n",
        "        if not content:\n",
        "            return False\n",
        "\n",
        "        # Get all client countries for this event from the Excel processor\n",
        "        event_clients = self.excel_processor.get_client_countries_for_event(gdacs_event.event_id)\n",
        "\n",
        "        if not event_clients:\n",
        "            # Fallback to the individual event's client_country field\n",
        "            if not gdacs_event.client_country:\n",
        "                return False\n",
        "            event_clients = [country.strip() for country in gdacs_event.client_country.split(',') if country.strip()]\n",
        "\n",
        "        if not event_clients:\n",
        "            return False\n",
        "\n",
        "        # Expand client country codes/addresses to full country names\n",
        "        expanded_country_names = self._expand_client_country_names(event_clients)\n",
        "\n",
        "        content_lower = content.lower()\n",
        "\n",
        "        # Check each expanded country name\n",
        "        for country_name in expanded_country_names:\n",
        "            if country_name and country_name in content_lower:\n",
        "                print(f\"            ✅ Client country match found: '{country_name}' in content (from {event_clients})\")\n",
        "                return True\n",
        "\n",
        "        print(f\"            ⚠️ No client country match found. Original: {event_clients}, Expanded: {expanded_country_names[:5]}\")\n",
        "        return False\n",
        "\n",
        "    def _create_fallback_analysis(self, gdacs_event: GDACSEvent) -> DisasterAnalysis:\n",
        "        \"\"\"Create fallback analysis when GPT fails\"\"\"\n",
        "        analysis = DisasterAnalysis()\n",
        "\n",
        "        analysis.disaster_severity_score = min(gdacs_event.alert_score * 30, 100) if gdacs_event.alert_score else 50\n",
        "        analysis.response_capability_score = 45.0\n",
        "        analysis.response_timeliness_score = 40.0\n",
        "        analysis.disaster_relevance_score = 70.0\n",
        "        analysis.gdacs_alignment_score = 60.0\n",
        "        analysis.media_coverage_score = 50.0\n",
        "        analysis.final_risk_score = 50.0\n",
        "        analysis.confidence_score = 30.0\n",
        "\n",
        "        analysis.is_disaster_related = True\n",
        "        analysis.primary_disaster_type = gdacs_event.event_type\n",
        "        analysis.gpt_analysis_summary = f\"Fallback analysis for {gdacs_event.event_name}\"\n",
        "        analysis.gdacs_context_analysis = f\"GDACS {gdacs_event.alert_level} alert event\"\n",
        "\n",
        "        analysis.severity_level = self._get_severity_level(analysis.disaster_severity_score)\n",
        "        analysis.response_level = self._get_response_level(analysis.response_capability_score)\n",
        "        analysis.timeliness_level = self._get_timeliness_level(analysis.response_timeliness_score)\n",
        "\n",
        "        return analysis\n",
        "\n",
        "    def _create_empty_result(self, row_index: int, gdacs_event: GDACSEvent) -> Dict:\n",
        "        \"\"\"Create empty result structure when no articles found\"\"\"\n",
        "        return {\n",
        "            'row_index': row_index,\n",
        "            'gdacs_event': gdacs_event,\n",
        "            'articles': [],\n",
        "            'analyses': [],\n",
        "            'summary': {\n",
        "                'total_articles': 0,\n",
        "                'avg_disaster_severity': 0,\n",
        "                'avg_response_capability': 0,\n",
        "                'avg_response_timeliness': 0,\n",
        "                'avg_disaster_relevance': 0,\n",
        "                'avg_gdacs_alignment': 0,\n",
        "                'avg_media_coverage': 0,\n",
        "                'avg_final_risk': 0,\n",
        "                'avg_confidence': 0,\n",
        "                'disaster_related_count': 0,\n",
        "                'client_country_matches': 0,\n",
        "                'avg_client_risk': 0,\n",
        "                'total_client_assessments': 0\n",
        "            },\n",
        "            'date_range': {'from_date': None, 'to_date': None},\n",
        "            'localization': {'lang': 'en', 'country': 'US'}\n",
        "        }\n",
        "\n",
        "    def _calculate_summary(self, analyses: List[DisasterAnalysis]) -> Dict:\n",
        "        \"\"\"Calculate summary statistics from analyses including client risk data\"\"\"\n",
        "        if not analyses:\n",
        "            return self._create_empty_result(0, None)['summary']\n",
        "\n",
        "        disaster_related = [a for a in analyses if a.is_disaster_related]\n",
        "        client_matches = [a for a in analyses if a.client_country_match]\n",
        "\n",
        "        # Calculate client risk statistics\n",
        "        all_client_risks = []\n",
        "        for analysis in analyses:\n",
        "            if analysis.client_risk_scores:\n",
        "                all_client_risks.extend(analysis.client_risk_scores.values())\n",
        "\n",
        "        return {\n",
        "            'total_articles': len(analyses),\n",
        "            'avg_disaster_severity': np.mean([a.disaster_severity_score for a in analyses]),\n",
        "            'avg_response_capability': np.mean([a.response_capability_score for a in analyses]),\n",
        "            'avg_response_timeliness': np.mean([a.response_timeliness_score for a in analyses]),\n",
        "            'avg_disaster_relevance': np.mean([a.disaster_relevance_score for a in analyses]),\n",
        "            'avg_gdacs_alignment': np.mean([a.gdacs_alignment_score for a in analyses]),\n",
        "            'avg_media_coverage': np.mean([a.media_coverage_score for a in analyses]),\n",
        "            'avg_final_risk': np.mean([a.final_risk_score for a in analyses]),\n",
        "            'avg_confidence': np.mean([a.confidence_score for a in analyses]),\n",
        "            'disaster_related_count': len(disaster_related),\n",
        "            'client_country_matches': len(client_matches),\n",
        "            'avg_client_risk': np.mean(all_client_risks) if all_client_risks else 0,\n",
        "            'total_client_assessments': len(all_client_risks)\n",
        "        }\n",
        "\n",
        "# ==========================================\n",
        "# EXCEL EXPORT FUNCTIONALITY WITH LOCALIZATION TRACKING (MODIFIED)\n",
        "# ==========================================\n",
        "\n",
        "def export_results_to_excel(results: List[Dict], filename: str = None) -> str:\n",
        "    \"\"\"Export comprehensive results to Excel with localization and client address tracking\"\"\"\n",
        "\n",
        "    if not filename:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = f\"disaster_analysis_localized_{timestamp}.xlsx\"\n",
        "\n",
        "    print(f\"📊 Exporting results to Excel: {filename}\")\n",
        "\n",
        "    # Prepare data rows for export\n",
        "    export_rows = []\n",
        "\n",
        "    for result in results:\n",
        "        gdacs_event = result['gdacs_event']\n",
        "        articles = result['articles']\n",
        "        analyses = result['analyses']\n",
        "        date_range = result.get('date_range', {})\n",
        "        localization = result.get('localization', {'lang': 'en', 'country': 'US'})\n",
        "\n",
        "        # Process each article-analysis pair\n",
        "        for i, (article, analysis) in enumerate(zip(articles, analyses)):\n",
        "            try:\n",
        "                # Basic row data\n",
        "                row = {\n",
        "                    # Article Information\n",
        "                    'title': str(article.get('title', ''))[:500],\n",
        "                    'url': str(article.get('link', '')),\n",
        "                    'date_text': str(article.get('published', '')),\n",
        "                    'parsed_date': str(article.get('parsed_date', '')),\n",
        "                    'summary': str(article.get('summary', ''))[:1000],\n",
        "                    'extraction_method': str(article.get('extraction_method', 'unknown')),\n",
        "                    'lang_country': str(article.get('lang_country', 'unknown')),  # NEW: Track localization\n",
        "\n",
        "                    # GDACS Event Information\n",
        "                    'gdacs_event_id': str(gdacs_event.event_id),\n",
        "                    'gdacs_event_type': str(gdacs_event.event_type),\n",
        "                    'gdacs_event_name': str(gdacs_event.event_name),\n",
        "                    'gdacs_alert_level': str(gdacs_event.alert_level),\n",
        "                    'gdacs_alert_score': float(gdacs_event.alert_score) if gdacs_event.alert_score else 0.0,\n",
        "                    'gdacs_country': str(gdacs_event.disaster_country),\n",
        "                    'gdacs_description': str(gdacs_event.event_description)[:500],\n",
        "                    'gdacs_severity_value': float(gdacs_event.severity_value) if gdacs_event.severity_value else 0.0,\n",
        "                    'gdacs_severity_text': str(gdacs_event.severity_text),\n",
        "                    'gdacs_from_date': str(gdacs_event.from_date),\n",
        "                    'gdacs_to_date': str(gdacs_event.to_date),\n",
        "\n",
        "                    # Date Range and Localization Information (NEW)\n",
        "                    'search_from_date': str(date_range.get('from_date', '')),\n",
        "                    'search_to_date': str(date_range.get('to_date', '')),\n",
        "                    'search_language': str(localization.get('lang', 'en')),\n",
        "                    'search_country': str(localization.get('country', 'US')),\n",
        "\n",
        "                    # Client Information\n",
        "                    'client_country': str(gdacs_event.client_country),\n",
        "                    'client_address': str(gdacs_event.client_address),\n",
        "                    'distance_km': float(gdacs_event.distance_km) if gdacs_event.distance_km else 0.0,\n",
        "\n",
        "                    # Analysis Scores\n",
        "                    'disaster_severity_score': float(analysis.disaster_severity_score),\n",
        "                    'response_capability_score': float(analysis.response_capability_score),\n",
        "                    'response_timeliness_score': float(analysis.response_timeliness_score),\n",
        "                    'disaster_relevance_score': float(analysis.disaster_relevance_score),\n",
        "                    'gdacs_alignment_score': float(analysis.gdacs_alignment_score),\n",
        "                    'media_coverage_score': float(analysis.media_coverage_score),\n",
        "                    'final_risk_score': float(analysis.final_risk_score),\n",
        "                    'confidence_score': float(analysis.confidence_score),\n",
        "\n",
        "                    # Analysis Results\n",
        "                    'is_disaster_related': bool(analysis.is_disaster_related),\n",
        "                    'severity_level': str(analysis.severity_level),\n",
        "                    'response_level': str(analysis.response_level),\n",
        "                    'timeliness_level': str(analysis.timeliness_level),\n",
        "                    'relevance_level': str(analysis.relevance_level),\n",
        "                    'gdacs_alignment_level': str(analysis.gdacs_alignment_level),\n",
        "                    'media_coverage_level': str(analysis.media_coverage_level),\n",
        "                    'primary_disaster_type': str(analysis.primary_disaster_type),\n",
        "                    'client_country_match': bool(analysis.client_country_match),\n",
        "\n",
        "                    # GPT Analysis Details\n",
        "                    'gpt_disaster_keywords': ', '.join(analysis.gpt_disaster_keywords),\n",
        "                    'gpt_response_keywords': ', '.join(analysis.gpt_response_keywords),\n",
        "                    'gpt_timeliness_keywords': ', '.join(analysis.gpt_timeliness_keywords),\n",
        "                    'gpt_analysis_summary': str(analysis.gpt_analysis_summary)[:500],\n",
        "                    'gdacs_context_analysis': str(analysis.gdacs_context_analysis)[:300],\n",
        "\n",
        "                    # Client Risk Scores with Addresses (MODIFIED)\n",
        "                    'client_risk_scores_json': json.dumps(analysis.client_risk_scores) if analysis.client_risk_scores else '{}',\n",
        "                    'matched_client_addresses': ', '.join(analysis.matched_client_addresses) if hasattr(analysis, 'matched_client_addresses') else '',\n",
        "                    'matched_location': str(analysis.matched_location) if hasattr(analysis, 'matched_location') else ''\n",
        "                }\n",
        "\n",
        "                # Add individual client risk scores using address names (MODIFIED)\n",
        "                if analysis.client_risk_scores:\n",
        "                    for client_address, risk_score in analysis.client_risk_scores.items():\n",
        "                        # Create a clean column name from the address\n",
        "                        clean_address = re.sub(r'[^\\w\\s]', '_', client_address)[:50]  # Limit length\n",
        "                        clean_address = re.sub(r'\\s+', '_', clean_address)\n",
        "                        column_name = f'risk_{clean_address}'\n",
        "                        row[column_name] = float(risk_score)\n",
        "\n",
        "                    risk_summary = []\n",
        "                    for address, score in analysis.client_risk_scores.items():\n",
        "                        short_address = address[:30] + \"...\" if len(address) > 30 else address\n",
        "                        risk_summary.append(f\"{short_address}: {score:.1f}\")\n",
        "                    row['client_risk_scores_summary'] = '; '.join(risk_summary)\n",
        "                else:\n",
        "                    row['client_risk_scores_summary'] = 'No client risk scores calculated'\n",
        "\n",
        "                export_rows.append(row)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Error processing article {i} for event {gdacs_event.event_id}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Create DataFrame and export\n",
        "    if not export_rows:\n",
        "        print(\"❌ No data to export\")\n",
        "        return None\n",
        "\n",
        "    df_export = pd.DataFrame(export_rows)\n",
        "\n",
        "    # Create summary statistics with localization info\n",
        "    summary_data = []\n",
        "    for result in results:\n",
        "        gdacs_event = result['gdacs_event']\n",
        "        analyses = result['analyses']\n",
        "        articles = result['articles']\n",
        "        date_range = result.get('date_range', {})\n",
        "        localization = result.get('localization', {'lang': 'en', 'country': 'US'})\n",
        "\n",
        "        if analyses:\n",
        "            # Count extraction methods\n",
        "            extraction_methods = [a.get('extraction_method', 'unknown') for a in articles]\n",
        "            gpt_success_count = sum(1 for method in extraction_methods if method == 'gpt_success')\n",
        "\n",
        "            # Calculate client risk statistics\n",
        "            all_client_risks = []\n",
        "            unique_clients = set()\n",
        "            for analysis in analyses:\n",
        "                if analysis.client_risk_scores:\n",
        "                    for address, score in analysis.client_risk_scores.items():\n",
        "                        all_client_risks.append(score)\n",
        "                        unique_clients.add(address)\n",
        "\n",
        "            summary_data.append({\n",
        "                'Event_ID': gdacs_event.event_id,\n",
        "                'Event_Name': gdacs_event.event_name,\n",
        "                'Event_Type': gdacs_event.event_type,\n",
        "                'Alert_Level': gdacs_event.alert_level,\n",
        "                'Alert_Score': gdacs_event.alert_score,\n",
        "                'Countries': gdacs_event.disaster_country,\n",
        "                'Search_From_Date': date_range.get('from_date', ''),\n",
        "                'Search_To_Date': date_range.get('to_date', ''),\n",
        "                'Search_Language': localization.get('lang', 'en'),  # NEW\n",
        "                'Search_Country': localization.get('country', 'US'),  # NEW\n",
        "                'Articles_Count': len(analyses),\n",
        "                'GPT_URL_Extractions': len(articles),\n",
        "                'GPT_URL_Successes': gpt_success_count,\n",
        "                'GPT_Success_Rate': f\"{(gpt_success_count/len(articles)*100):.1f}%\" if articles else \"0%\",\n",
        "                'Avg_Disaster_Severity': np.mean([a.disaster_severity_score for a in analyses]),\n",
        "                'Avg_Response_Capability': np.mean([a.response_capability_score for a in analyses]),\n",
        "                'Avg_Response_Timeliness': np.mean([a.response_timeliness_score for a in analyses]),\n",
        "                'Avg_Final_Risk': np.mean([a.final_risk_score for a in analyses]),\n",
        "                'Avg_Confidence': np.mean([a.confidence_score for a in analyses]),\n",
        "                'Client_Matches': len([a for a in analyses if a.client_country_match]),\n",
        "                'Unique_Client_Addresses': len(unique_clients),  # MODIFIED: Changed from countries to addresses\n",
        "                'Client_Addresses_List': ', '.join([addr[:30] + \"...\" if len(addr) > 30 else addr for addr in sorted(unique_clients)]) if unique_clients else 'None',\n",
        "                'Avg_Client_Risk_Score': np.mean(all_client_risks) if all_client_risks else 0.0,\n",
        "                'Max_Client_Risk_Score': max(all_client_risks) if all_client_risks else 0.0,\n",
        "                'Min_Client_Risk_Score': min(all_client_risks) if all_client_risks else 0.0,\n",
        "                'Total_Client_Risk_Assessments': len(all_client_risks)\n",
        "            })\n",
        "\n",
        "    df_summary = pd.DataFrame(summary_data) if summary_data else pd.DataFrame()\n",
        "\n",
        "    # Create client risk details sheet with addresses\n",
        "    client_risk_details = []\n",
        "    for result in results:\n",
        "        gdacs_event = result['gdacs_event']\n",
        "        for i, analysis in enumerate(result['analyses']):\n",
        "            if analysis.client_risk_scores:\n",
        "                for client_address, risk_score in analysis.client_risk_scores.items():\n",
        "                    client_risk_details.append({\n",
        "                        'Event_ID': gdacs_event.event_id,\n",
        "                        'Event_Name': gdacs_event.event_name,\n",
        "                        'Event_Type': gdacs_event.event_type,\n",
        "                        'Alert_Level': gdacs_event.alert_level,\n",
        "                        'Article_Index': i,\n",
        "                        'Client_Address': client_address,  # MODIFIED: Full address instead of country code\n",
        "                        'Risk_Score': float(risk_score),\n",
        "                        'Risk_Level': 'Critical' if risk_score >= 8 else 'High' if risk_score >= 6 else 'Medium' if risk_score >= 4 else 'Low'\n",
        "                    })\n",
        "\n",
        "    df_client_risks = pd.DataFrame(client_risk_details) if client_risk_details else pd.DataFrame()\n",
        "\n",
        "    # Export to Excel with multiple sheets\n",
        "    try:\n",
        "        with pd.ExcelWriter(filename, engine='openpyxl') as writer:\n",
        "            # Main analysis results\n",
        "            df_export.to_excel(writer, sheet_name='Disaster_Analysis', index=False)\n",
        "\n",
        "            # Summary statistics\n",
        "            if not df_summary.empty:\n",
        "                df_summary.to_excel(writer, sheet_name='Summary_Statistics', index=False)\n",
        "\n",
        "            # Client risk details\n",
        "            if not df_client_risks.empty:\n",
        "                df_client_risks.to_excel(writer, sheet_name='Client_Risk_Details', index=False)\n",
        "                print(f\"✅ Client Risk Details sheet created with {len(df_client_risks)} risk assessments using addresses\")\n",
        "\n",
        "            # Metadata sheet\n",
        "            metadata = {\n",
        "                'Export_Date': [datetime.now().strftime('%Y-%m-%d %H:%M:%S')],\n",
        "                'Total_Articles': [len(df_export)],\n",
        "                'Total_Events': [len(results)],\n",
        "                'Total_Client_Risk_Assessments': [len(df_client_risks)] if not df_client_risks.empty else [0],\n",
        "                'System_Version': ['Enhanced Disaster Analysis v3.3 - PyGoogleNews with Localization'],\n",
        "                'Description': ['GDACS Event Media Analysis with PyGoogleNews, GPT Country/Language Detection, Localized Search, and Client Address Risk Scoring'],\n",
        "                'News_API': ['PyGoogleNews with country/language localization'],\n",
        "                'Localization_Features': ['GPT-powered country/language detection, Localized disaster terminology, Client address-based risk scoring'],\n",
        "                'Date_Range_Feature': ['Automatic extraction from Excel From_Date/To_Date for targeted news search'],\n",
        "                'New_Features': ['PyGoogleNews API integration, GPT language localization, Client address risk scoring instead of country codes']\n",
        "            }\n",
        "            pd.DataFrame(metadata).to_excel(writer, sheet_name='Export_Info', index=False)\n",
        "\n",
        "        print(f\"✅ Successfully exported {len(df_export)} article analyses with localization\")\n",
        "        print(f\"🌍 Localization: PyGoogleNews with GPT country/language detection\")\n",
        "        print(f\"📊 Client Risk Scores: {len(df_client_risks)} assessments using client addresses\" if client_risk_details else \"⚠️ No client risk scores found\")\n",
        "        return filename\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error creating Excel file: {e}\")\n",
        "\n",
        "        # Fallback to CSV\n",
        "        csv_filename = filename.replace('.xlsx', '.csv')\n",
        "        df_export.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "        print(f\"📄 Exported to CSV instead: {csv_filename}\")\n",
        "        return csv_filename\n",
        "\n",
        "# ==========================================\n",
        "# MAIN EXECUTION FUNCTIONS (UPDATED)\n",
        "# ==========================================\n",
        "\n",
        "def run_enhanced_disaster_analysis():\n",
        "    \"\"\"Main interactive function for the enhanced disaster analysis system with PyGoogleNews localization\"\"\"\n",
        "\n",
        "    print(\"ENHANCED DISASTER NEWS ANALYSIS SYSTEM v3.3\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Complete system with PyGoogleNews, GPT localization, client address risk scoring\")\n",
        "    print(\"NEW: PyGoogleNews API with GPT-powered country/language detection and localized search\")\n",
        "    print(\"Features: Localized news search, GPT country analysis, Client address risk assessment, Excel export\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Get API key\n",
        "    api_key = input(\"\\nEnter your OpenAI API key: \").strip()\n",
        "    if not api_key:\n",
        "        print(\"API key is required. Exiting...\")\n",
        "        return\n",
        "\n",
        "    # Get Excel file path\n",
        "    excel_path = input(\"\\nEnter Excel file path (or press Enter for default): \").strip()\n",
        "    if not excel_path:\n",
        "        excel_path = \"enhanced_disaster_analysis_400.0km_buffer10.0km 2.xlsx\"\n",
        "\n",
        "    try:\n",
        "        # Initialize analyzer\n",
        "        print(\"\\nInitializing Enhanced Disaster Analyzer with PyGoogleNews localization...\")\n",
        "        analyzer = EnhancedDisasterAnalyzer(api_key, excel_path)\n",
        "\n",
        "        if analyzer.excel_processor.get_total_rows() == 0:\n",
        "            print(\"No data found in Excel file. Please check the file path and format.\")\n",
        "            return\n",
        "\n",
        "        print(\"✅ PyGoogleNews Localization: System will detect client country language and localize search terms\")\n",
        "        print(\"🌍 Client address-based risk scoring replaces country codes\")\n",
        "\n",
        "        stored_results = []\n",
        "\n",
        "        # Main menu loop\n",
        "        while True:\n",
        "            print(\"\\n\" + \"=\" * 60)\n",
        "            print(\"ENHANCED DISASTER ANALYSIS MENU - PYGOOGLENEWS LOCALIZED VERSION\")\n",
        "            print(\"=\" * 60)\n",
        "            print(\"1. Process single row\")\n",
        "            print(\"2. Process batch of rows\")\n",
        "            print(\"3. Quick demo (first 3 rows)\")\n",
        "            print(\"4. Export results to Excel\")\n",
        "            print(\"5. System statistics\")\n",
        "            print(\"6. Exit\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            choice = input(\"\\nSelect option (1-6): \").strip()\n",
        "\n",
        "            if choice == '1':\n",
        "                # Single row processing\n",
        "                total_rows = analyzer.excel_processor.get_total_rows()\n",
        "                print(f\"\\nTotal rows available: {total_rows}\")\n",
        "\n",
        "                try:\n",
        "                    row_index = int(input(f\"Enter row index (0-{total_rows-1}): \"))\n",
        "                    if 0 <= row_index < total_rows:\n",
        "                        result = analyzer.process_single_row(row_index)\n",
        "                        if result:\n",
        "                            stored_results = [result]\n",
        "                            print(f\"\\nSingle row analysis completed!\")\n",
        "                            print(f\"Articles found: {len(result['articles'])}\")\n",
        "                            print(f\"Localization: {result['localization']['lang']}-{result['localization']['country']}\")\n",
        "                            print(f\"Analyses completed: {len(result['analyses'])}\")\n",
        "                            print(f\"Date range used: {result['date_range']['from_date']} to {result['date_range']['to_date']}\")\n",
        "\n",
        "                            summary = result['summary']\n",
        "                            print(f\"Average severity score: {summary['avg_disaster_severity']:.1f}\")\n",
        "                            print(f\"Average response score: {summary['avg_response_capability']:.1f}\")\n",
        "                            print(f\"Average client risk: {summary['avg_client_risk']:.1f}\")\n",
        "                        else:\n",
        "                            print(\"Single row processing failed\")\n",
        "                    else:\n",
        "                        print(f\"Invalid row index. Must be 0-{total_rows-1}\")\n",
        "                except ValueError:\n",
        "                    print(\"Please enter a valid number\")\n",
        "\n",
        "            elif choice == '2':\n",
        "                # Batch processing\n",
        "                total_rows = analyzer.excel_processor.get_total_rows()\n",
        "                print(f\"\\nTotal rows available: {total_rows}\")\n",
        "\n",
        "                try:\n",
        "                    start_row = int(input(f\"Enter start row (0-{total_rows-1}): \"))\n",
        "                    num_rows = int(input(\"Enter number of rows to process: \"))\n",
        "\n",
        "                    if 0 <= start_row < total_rows and num_rows > 0:\n",
        "                        print(f\"\\nProcessing {num_rows} rows starting from row {start_row}...\")\n",
        "                        print(\"🌍 Using PyGoogleNews with GPT localization and client address risk scoring\")\n",
        "                        results = analyzer.process_batch_rows(start_row, num_rows)\n",
        "\n",
        "                        if results:\n",
        "                            stored_results = results\n",
        "                            print(f\"\\nBatch processing completed!\")\n",
        "                            print(f\"Processed {len(results)} rows successfully\")\n",
        "                            print(f\"Localizations: {analyzer.stats['country_localizations']} countries, {analyzer.stats['term_localizations']} term sets\")\n",
        "\n",
        "                            # Auto-export to Excel\n",
        "                            print(f\"\\nExporting results to Excel...\")\n",
        "                            filename = export_results_to_excel(results)\n",
        "                            if filename:\n",
        "                                print(f\"Results saved to: {filename}\")\n",
        "                        else:\n",
        "                            print(\"Batch processing failed\")\n",
        "                    else:\n",
        "                        print(\"Invalid parameters\")\n",
        "                except ValueError:\n",
        "                    print(\"Please enter valid numbers\")\n",
        "\n",
        "            elif choice == '3':\n",
        "                # Quick demo\n",
        "                print(\"\\nRunning quick demo (first 3 rows) with PyGoogleNews localization...\")\n",
        "                results = analyzer.process_batch_rows(0, 3)\n",
        "\n",
        "                if results:\n",
        "                    stored_results = results\n",
        "                    print(f\"\\nDemo completed! Processed {len(results)} rows\")\n",
        "\n",
        "                    for result in results:\n",
        "                        event = result['gdacs_event']\n",
        "                        summary = result['summary']\n",
        "                        date_range = result['date_range']\n",
        "                        localization = result['localization']\n",
        "                        print(f\"\\nEvent: {event.event_name}\")\n",
        "                        print(f\"  Type: {event.event_type} | Alert: {event.alert_level}\")\n",
        "                        print(f\"  Client: {event.client_country} | Address: {event.client_address[:30]}...\")\n",
        "                        print(f\"  Localization: {localization['lang']}-{localization['country']}\")\n",
        "                        print(f\"  Search dates: {date_range['from_date']} to {date_range['to_date']}\")\n",
        "                        print(f\"  Articles: {summary['total_articles']}\")\n",
        "                        print(f\"  Avg Risk Score: {summary['avg_final_risk']:.1f}\")\n",
        "\n",
        "                    # Export demo results\n",
        "                    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "                    filename = export_results_to_excel(results, f\"demo_localized_{timestamp}.xlsx\")\n",
        "                    if filename:\n",
        "                        print(f\"Demo results saved to: {filename}\")\n",
        "                else:\n",
        "                    print(\"Demo failed\")\n",
        "\n",
        "            elif choice == '4':\n",
        "                # Export to Excel\n",
        "                if stored_results:\n",
        "                    print(f\"\\nExporting {len(stored_results)} processed results to Excel...\")\n",
        "                    custom_filename = input(\"Enter filename (or press Enter for auto-generated): \").strip()\n",
        "\n",
        "                    if custom_filename and not custom_filename.endswith('.xlsx'):\n",
        "                        custom_filename += '.xlsx'\n",
        "\n",
        "                    filename = export_results_to_excel(stored_results, custom_filename or None)\n",
        "                    if filename:\n",
        "                        print(f\"Results exported successfully to: {filename}\")\n",
        "                        print(f\"Format: Complete analysis with PyGoogleNews localization and client address risk scoring\")\n",
        "                else:\n",
        "                    print(\"No results available for export. Please process some data first.\")\n",
        "\n",
        "            elif choice == '5':\n",
        "                # System statistics\n",
        "                print(f\"\\nSYSTEM STATISTICS - PYGOOGLENEWS LOCALIZED VERSION\")\n",
        "                print(\"=\" * 50)\n",
        "                print(f\"Rows processed: {analyzer.stats['rows_processed']}\")\n",
        "                print(f\"Articles fetched: {analyzer.stats['articles_fetched']}\")\n",
        "                print(f\"Country localizations: {analyzer.stats['country_localizations']}\")\n",
        "                print(f\"Term localizations: {analyzer.stats['term_localizations']}\")\n",
        "                print(f\"GPT URL extractions attempted: {analyzer.stats['gpt_url_extractions']}\")\n",
        "                print(f\"GPT URL extractions successful: {analyzer.stats['gpt_url_successes']}\")\n",
        "                print(f\"GPT URL success rate: {(analyzer.stats['gpt_url_successes']/analyzer.stats['gpt_url_extractions']*100):.1f}%\" if analyzer.stats['gpt_url_extractions'] > 0 else \"0%\")\n",
        "                print(f\"GPT analyses: {analyzer.stats['gpt_analyses']}\")\n",
        "                print(f\"Client risk calculations: {analyzer.stats['client_risk_calculations']}\")\n",
        "                print(f\"API calls made: {analyzer.stats['api_calls']}\")\n",
        "                print(f\"Successful analyses: {analyzer.stats['successful_analyses']}\")\n",
        "                print(f\"Errors encountered: {analyzer.stats['errors']}\")\n",
        "                print(f\"Results in memory: {len(stored_results)} events\")\n",
        "                print(f\"News API: PyGoogleNews with localization\")\n",
        "                print(f\"Risk Scoring: Client address-based (not country codes)\")\n",
        "\n",
        "                elapsed = time.time() - analyzer.stats['start_time']\n",
        "                print(f\"Total runtime: {elapsed/60:.1f} minutes\")\n",
        "\n",
        "            elif choice == '6':\n",
        "                print(\"\\nExiting Enhanced Disaster Analysis System\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(\"\\nInvalid choice. Please select 1-6.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nSystem error: {e}\")\n",
        "        print(\"Please check your inputs and try again\")\n",
        "\n",
        "def quick_demo_analysis():\n",
        "    \"\"\"Quick demonstration of system capabilities with PyGoogleNews localization\"\"\"\n",
        "\n",
        "    print(\"QUICK DEMO - Enhanced Disaster Analysis System v3.3\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    api_key = input(\"Enter OpenAI API key for demo: \").strip()\n",
        "    if not api_key:\n",
        "        print(\"API key required for demo\")\n",
        "        return\n",
        "\n",
        "    excel_path = \"enhanced_disaster_analysis_400.0km_buffer10.0km 2.xlsx\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\nInitializing system for demo with PyGoogleNews localization...\")\n",
        "        analyzer = EnhancedDisasterAnalyzer(api_key, excel_path)\n",
        "\n",
        "        if analyzer.excel_processor.get_total_rows() == 0:\n",
        "            print(\"No data found. Please check Excel file path.\")\n",
        "            return\n",
        "\n",
        "        print(\"\\nRunning demo analysis on first 2 rows...\")\n",
        "        print(\"News API: PyGoogleNews with GPT country/language detection\")\n",
        "        print(\"Risk Scoring: Client addresses instead of country codes\")\n",
        "        results = analyzer.process_batch_rows(0, 2)\n",
        "\n",
        "        if results:\n",
        "            print(f\"\\nDemo completed successfully!\")\n",
        "            print(f\"Processed {len(results)} events\")\n",
        "\n",
        "            for i, result in enumerate(results, 1):\n",
        "                event = result['gdacs_event']\n",
        "                summary = result['summary']\n",
        "                date_range = result['date_range']\n",
        "                localization = result['localization']\n",
        "\n",
        "                print(f\"\\nEVENT {i}: {event.event_name}\")\n",
        "                print(f\"  Type: {event.event_type} | Alert: {event.alert_level}\")\n",
        "                print(f\"  Country: {event.disaster_country}\")\n",
        "                print(f\"  Client Address: {event.client_address[:50]}...\")\n",
        "                print(f\"  Localization Used: {localization['lang']}-{localization['country']}\")\n",
        "                print(f\"  Search From_Date: {date_range['from_date']}\")\n",
        "                print(f\"  Search To_Date: {date_range['to_date']}\")\n",
        "                print(f\"  Articles Found: {summary['total_articles']}\")\n",
        "                print(f\"  Disaster Severity: {summary['avg_disaster_severity']:.1f}/100\")\n",
        "                print(f\"  Response Capability: {summary['avg_response_capability']:.1f}/100\")\n",
        "                print(f\"  Final Risk Score: {summary['avg_final_risk']:.1f}/100\")\n",
        "                print(f\"  Client Risk Score: {summary['avg_client_risk']:.1f}/10\")\n",
        "\n",
        "            print(f\"\\nDEMO STATISTICS:\")\n",
        "            print(f\"API calls: {analyzer.stats['api_calls']}\")\n",
        "            print(f\"Country localizations: {analyzer.stats['country_localizations']}\")\n",
        "            print(f\"Term localizations: {analyzer.stats['term_localizations']}\")\n",
        "            print(f\"GPT URL analysis success rate: {(analyzer.stats['gpt_url_successes']/analyzer.stats['gpt_url_extractions']*100):.1f}%\" if analyzer.stats['gpt_url_extractions'] > 0 else \"0%\")\n",
        "            print(f\"Client risk calculations: {analyzer.stats['client_risk_calculations']}\")\n",
        "            print(f\"Processing time: {(time.time() - analyzer.stats['start_time'])/60:.1f} minutes\")\n",
        "            print(f\"News API: PyGoogleNews with localization\")\n",
        "            print(f\"Risk Scoring: Client address-based\")\n",
        "\n",
        "            # Export demo results\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = export_results_to_excel(results, f\"demo_localized_{timestamp}.xlsx\")\n",
        "            if filename:\n",
        "                print(f\"Demo results exported to: {filename}\")\n",
        "                print(\"Export includes PyGoogleNews localization tracking and client address risk data\")\n",
        "\n",
        "            print(f\"\\nDemo completed! System ready for full analysis with PyGoogleNews localization.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Demo failed - no results generated\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Demo error: {e}\")\n",
        "\n",
        "def batch_analysis(api_key: str, excel_path: str, start_row: int = 0, num_rows: int = None) -> Optional[Dict]:\n",
        "    \"\"\"Programmatic batch analysis function for external use with PyGoogleNews localization\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(f\"Starting batch analysis with PyGoogleNews localization...\")\n",
        "        print(f\"Excel file: {excel_path}\")\n",
        "        print(f\"Start row: {start_row}\")\n",
        "        print(f\"Number of rows: {num_rows or 'All remaining'}\")\n",
        "        print(f\"News API: PyGoogleNews with GPT country/language detection\")\n",
        "        print(f\"Risk Scoring: Client address-based\")\n",
        "\n",
        "        analyzer = EnhancedDisasterAnalyzer(api_key, excel_path)\n",
        "        total_rows = analyzer.excel_processor.get_total_rows()\n",
        "\n",
        "        if total_rows == 0:\n",
        "            print(\"No data found in Excel file\")\n",
        "            return None\n",
        "\n",
        "        if num_rows is None:\n",
        "            num_rows = total_rows - start_row\n",
        "\n",
        "        if start_row >= total_rows:\n",
        "            print(f\"Start row {start_row} exceeds total rows {total_rows}\")\n",
        "            return None\n",
        "\n",
        "        print(f\"Processing {num_rows} rows starting from row {start_row}...\")\n",
        "        results = analyzer.process_batch_rows(start_row, num_rows)\n",
        "\n",
        "        if not results:\n",
        "            print(\"Batch analysis failed - no results\")\n",
        "            return None\n",
        "\n",
        "        # Export results automatically\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        filename = export_results_to_excel(results, f\"batch_localized_{timestamp}.xlsx\")\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': analyzer.stats,\n",
        "            'total_processed': len(results),\n",
        "            'processing_time': (time.time() - analyzer.stats['start_time']) / 60,\n",
        "            'export_filename': filename,\n",
        "            'country_localizations': analyzer.stats['country_localizations'],\n",
        "            'term_localizations': analyzer.stats['term_localizations'],\n",
        "            'gpt_url_extractions': analyzer.stats['gpt_url_extractions'],\n",
        "            'gpt_url_successes': analyzer.stats['gpt_url_successes'],\n",
        "            'gpt_url_success_rate': (analyzer.stats['gpt_url_successes']/analyzer.stats['gpt_url_extractions']*100) if analyzer.stats['gpt_url_extractions'] > 0 else 0\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Batch analysis error: {e}\")\n",
        "        return None\n",
        "\n",
        "# ==========================================\n",
        "# SYSTEM VALIDATION AND HEALTH CHECK (UPDATED)\n",
        "# ==========================================\n",
        "\n",
        "def validate_system_requirements():\n",
        "    \"\"\"Validate that all required packages are available\"\"\"\n",
        "    required_packages = [\n",
        "        'openai', 'pygooglenews', 'beautifulsoup4', 'pandas',\n",
        "        'plotly', 'requests', 'lxml', 'openpyxl', 'numpy'\n",
        "    ]\n",
        "\n",
        "    missing_packages = []\n",
        "    for package in required_packages:\n",
        "        try:\n",
        "            __import__(package.replace('-', '_'))\n",
        "        except ImportError:\n",
        "            missing_packages.append(package)\n",
        "\n",
        "    if missing_packages:\n",
        "        print(f\"Missing required packages: {', '.join(missing_packages)}\")\n",
        "        print(\"Please install them using: pip install \" + ' '.join(missing_packages))\n",
        "        return False\n",
        "\n",
        "    print(\"All required packages are available\")\n",
        "    return True\n",
        "\n",
        "def system_health_check():\n",
        "    \"\"\"Perform comprehensive system health check\"\"\"\n",
        "    print(\"SYSTEM HEALTH CHECK - Enhanced Disaster Analysis System v3.3\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    packages_ok = validate_system_requirements()\n",
        "\n",
        "    # Check for sample Excel file\n",
        "    excel_file = \"enhanced_disaster_analysis_400.0km_buffer10.0km 2.xlsx\"\n",
        "    excel_exists = os.path.exists(excel_file)\n",
        "    print(f\"\\nExcel File Status: {'✓ Found' if excel_exists else '✗ Missing'} ({excel_file})\")\n",
        "\n",
        "    # Test internet connection\n",
        "    try:\n",
        "        response = requests.get(\"https://news.google.com\", timeout=10)\n",
        "        internet_ok = response.status_code == 200\n",
        "        print(f\"Internet Connection: {'✓ OK' if internet_ok else '✗ Failed'}\")\n",
        "    except:\n",
        "        internet_ok = False\n",
        "        print(f\"Internet Connection: ✗ Failed\")\n",
        "\n",
        "    # Test PyGoogleNews initialization\n",
        "    try:\n",
        "        test_gn = GoogleNews(lang='en', country='US')\n",
        "        pygooglenews_ok = True\n",
        "        print(f\"PyGoogleNews API: ✓ OK\")\n",
        "    except:\n",
        "        pygooglenews_ok = False\n",
        "        print(f\"PyGoogleNews API: ✗ Failed\")\n",
        "\n",
        "    # Test date parsing functionality\n",
        "    test_dates = [\"2024-01-15\", \"15/01/2024\", \"01-15-2024\"]\n",
        "    date_parsing_ok = True\n",
        "    for test_date in test_dates:\n",
        "        if parse_date_input(test_date) is None:\n",
        "            date_parsing_ok = False\n",
        "            break\n",
        "    print(f\"Date Parsing: {'✓ OK' if date_parsing_ok else '✗ Failed'}\")\n",
        "\n",
        "    overall_health = packages_ok and internet_ok and pygooglenews_ok and date_parsing_ok\n",
        "    print(f\"\\nOVERALL SYSTEM HEALTH: {'✓ GOOD' if overall_health else '✗ ISSUES DETECTED'}\")\n",
        "    print(f\"PyGoogleNews Localization: {'✓ ACTIVE' if pygooglenews_ok else '✗ INACTIVE'}\")\n",
        "    print(f\"GPT Country/Language Detection: ✓ ACTIVE\")\n",
        "    print(f\"Client Address Risk Scoring: ✓ ACTIVE\")\n",
        "    print(f\"DATE RANGE INTEGRATION: ✓ ACTIVE (Excel From_Date/To_Date)\")\n",
        "\n",
        "    if not overall_health:\n",
        "        print(\"\\nRECOMMENDATIONS:\")\n",
        "        if not packages_ok:\n",
        "            print(\"   - Install missing packages with pip install\")\n",
        "        if not internet_ok:\n",
        "            print(\"   - Check internet connection for news search\")\n",
        "        if not pygooglenews_ok:\n",
        "            print(\"   - Install PyGoogleNews: pip install pygooglenews\")\n",
        "        if not excel_exists:\n",
        "            print(\"   - Ensure Excel file is in the correct location\")\n",
        "        if not date_parsing_ok:\n",
        "            print(\"   - Date parsing functionality issue detected\")\n",
        "\n",
        "    return overall_health\n",
        "\n",
        "# ==========================================\n",
        "# STARTUP AND MAIN EXECUTION (UPDATED)\n",
        "# ==========================================\n",
        "\n",
        "# System validation on load\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENHANCED DISASTER NEWS ANALYSIS SYSTEM v3.3 - PYGOOGLENEWS WITH LOCALIZATION\")\n",
        "print(\"=\"*80)\n",
        "print(\"Features: PyGoogleNews API • GPT Country/Language Detection • Localized Search • Client Address Risk Assessment\")\n",
        "print(\"NEW: PyGoogleNews replaces RSS feeds with GPT-powered localization and client address-based risk scoring\")\n",
        "print(\"Capabilities: Localized news search • GPT country analysis • Client address risk scoring • Comprehensive Excel export\")\n",
        "print(\"Status: Production Ready • PyGoogleNews Active • GPT Localization Active • Client Address Risk Scoring Active\")\n",
        "print(\"=\"*80)\n",
        "print(\"Quick Start: run_enhanced_disaster_analysis()\")\n",
        "print(\"Demo Mode: quick_demo_analysis()\")\n",
        "print(\"Batch Mode: batch_analysis(api_key, excel_path, start_row, num_rows)\")\n",
        "print(\"Health Check: system_health_check()\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Main execution block\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"Main entry point for the disaster analysis system\"\"\"\n",
        "    import sys\n",
        "\n",
        "    print(\"ENHANCED DISASTER NEWS ANALYSIS SYSTEM v3.3\")\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Complete system with PyGoogleNews, GPT localization, and client address risk scoring\")\n",
        "    print(\"PyGoogleNews API with GPT country/language detection and localized disaster terminology\")\n",
        "    print(\"Automatic From_Date/To_Date extraction from Excel for targeted news search\")\n",
        "    print(\"Client address-based risk assessment replacing country codes\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Handle command line arguments\n",
        "    if len(sys.argv) > 1:\n",
        "        if sys.argv[1] == \"--demo\":\n",
        "            quick_demo_analysis()\n",
        "        elif sys.argv[1] == \"--batch\":\n",
        "            if len(sys.argv) >= 4:\n",
        "                api_key = sys.argv[2]\n",
        "                excel_path = sys.argv[3]\n",
        "                start_row = int(sys.argv[4]) if len(sys.argv) > 4 else 0\n",
        "                num_rows = int(sys.argv[5]) if len(sys.argv) > 5 else None\n",
        "\n",
        "                result = batch_analysis(api_key, excel_path, start_row, num_rows)\n",
        "                if result:\n",
        "                    print(\"Batch analysis completed successfully!\")\n",
        "                    print(f\"Processed {result['total_processed']} rows in {result['processing_time']:.1f} minutes\")\n",
        "                    print(f\"Localizations: {result['country_localizations']} countries, {result['term_localizations']} term sets\")\n",
        "                    print(f\"GPT URL Analysis: {result['gpt_url_successes']}/{result['gpt_url_extractions']} successful ({result['gpt_url_success_rate']:.1f}%)\")\n",
        "                    print(f\"Results exported to: {result['export_filename']}\")\n",
        "                else:\n",
        "                    print(\"Batch analysis failed\")\n",
        "            else:\n",
        "                print(\"Usage: python script.py --batch <api_key> <excel_path> [start_row] [num_rows]\")\n",
        "        elif sys.argv[1] == \"--health\":\n",
        "            system_health_check()\n",
        "        elif sys.argv[1] == \"--help\":\n",
        "            print(\"\"\"\n",
        "DISASTER ANALYSIS SYSTEM HELP v3.3 - PYGOOGLENEWS WITH LOCALIZATION\n",
        "====================================================================\n",
        "\n",
        "USAGE OPTIONS:\n",
        "1. Interactive Mode (Default):\n",
        "   python disaster_analysis.py\n",
        "\n",
        "2. Demo Mode:\n",
        "   python disaster_analysis.py --demo\n",
        "\n",
        "3. Batch Processing:\n",
        "   python disaster_analysis.py --batch API_KEY excel_file.xlsx [start_row] [num_rows]\n",
        "\n",
        "4. Health Check:\n",
        "   python disaster_analysis.py --health\n",
        "\n",
        "5. Help:\n",
        "   python disaster_analysis.py --help\n",
        "\n",
        "NEW FEATURES IN v3.3:\n",
        "- PyGoogleNews API: Replaces RSS-based Google News search with proper API\n",
        "- GPT Country/Language Detection: AI-powered analysis of client countries and languages\n",
        "- Localized Search Terms: GPT translates disaster terms to local languages (e.g., \"huracán\" in Mexico)\n",
        "- Client Address Risk Scoring: Uses full client addresses instead of country codes in reports\n",
        "- Enhanced Localization: Supports 20+ languages with proper country/language combinations\n",
        "\n",
        "PYGOOGLENEWS LOCALIZATION FUNCTIONALITY:\n",
        "- Analyzes Client_Country and Client_Address to determine appropriate language/country\n",
        "- Uses GPT to translate disaster terms to local terminology\n",
        "- Initializes PyGoogleNews with correct lang/country parameters\n",
        "- Searches with localized terms for better local news coverage\n",
        "- Tracks localization usage in Excel exports\n",
        "\n",
        "CLIENT ADDRESS RISK SCORING:\n",
        "- Risk scores now use full Client_Address names instead of codes like \"MX_1\"\n",
        "- Provides clear geographical context for risk assessment\n",
        "- Excel export shows actual warehouse/facility addresses\n",
        "- Better identification of specific locations at risk\n",
        "\n",
        "EXAMPLES:\n",
        "- Run demo with localization: python disaster_analysis.py --demo\n",
        "- Process all rows with localization: python disaster_analysis.py --batch sk-xxx... data.xlsx\n",
        "- Process rows 5-15: python disaster_analysis.py --batch sk-xxx... data.xlsx 5 10\n",
        "- Check system including PyGoogleNews: python disaster_analysis.py --health\n",
        "\n",
        "REQUIREMENTS:\n",
        "- OpenAI API key (for GPT localization and analysis)\n",
        "- Excel file with Client_Country, Client_Address, From_Date and To_Date columns\n",
        "- Internet connection for PyGoogleNews API\n",
        "- pip install pygooglenews\n",
        "\n",
        "LOCALIZATION PROCESS:\n",
        "1. Read Client_Country and Client_Address from Excel\n",
        "2. Use GPT to determine appropriate language and country codes\n",
        "3. Initialize PyGoogleNews with localized settings\n",
        "4. Translate disaster terms to local language using GPT\n",
        "5. Search with localized terms for better regional coverage\n",
        "6. Track localization methods in Excel export\n",
        "\n",
        "EXCEL OUTPUT ENHANCEMENTS:\n",
        "- search_language and search_country columns show localization used\n",
        "- client_risk_scores use actual Client_Address names\n",
        "- Client_Addresses_List replaces Client_Countries_List in summary\n",
        "- Localization tracking in metadata sheet\n",
        "\n",
        "SUPPORTED LANGUAGES:\n",
        "English, Spanish, Chinese, Japanese, Korean, French, German, Italian, Portuguese,\n",
        "Russian, Arabic, Hindi, Thai, Vietnamese, Dutch, Swedish, Danish, Norwegian,\n",
        "Finnish, Polish, Turkish, and more via GPT detection\n",
        "\n",
        "SYSTEM ADVANTAGES:\n",
        "- Better local news coverage through proper localization\n",
        "- More accurate disaster terminology for each region\n",
        "- Clear client address identification in risk reports\n",
        "- Improved cultural and linguistic relevance\n",
        "- Future-proof API-based news access\n",
        "            \"\"\")\n",
        "        else:\n",
        "            print(f\"Unknown option: {sys.argv[1]}. Use --help for available options.\")\n",
        "    else:\n",
        "        # Run interactive system\n",
        "        run_enhanced_disaster_analysis()\n",
        "\n",
        "# System validation when imported as module\n",
        "if __name__ != \"__main__\":\n",
        "    try:\n",
        "        validate_system_requirements()\n",
        "        print(\"Enhanced Disaster News Analysis System v3.3 loaded successfully!\")\n",
        "        print(\"PyGoogleNews Localization: Active - GPT-powered country/language detection and localized search\")\n",
        "        print(\"Client Address Risk Scoring: Active - Full addresses replace country codes in reports\")\n",
        "        print(\"Date Range Integration: Active - Automatic extraction from Excel From_Date/To_Date\")\n",
        "        print(\"System uses PyGoogleNews API with GPT localization for superior regional news coverage\")\n",
        "        print(\"Ready to analyze disaster news with complete localization, client address risk assessment, and GDACS intelligence\")\n",
        "        print(\"Use run_enhanced_disaster_analysis() to start the interactive system\")\n",
        "        print(\"Or use batch_analysis() for programmatic processing\")\n",
        "        print(\"System validated and ready for production use with PyGoogleNews localization!\")\n",
        "    except Exception as e:\n",
        "        print(f\"System validation warning: {e}\")\n",
        "        print(\"System may still function, but some features might be limited\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kknVTY24jkQ_",
        "outputId": "403a4b61-551a-4cd1-b9b7-0b0863da87a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENHANCED DISASTER NEWS ANALYSIS SYSTEM v3.3\n",
            "================================================================================\n",
            "Complete system with PyGoogleNews, GPT localization, client address risk scoring\n",
            "NEW: PyGoogleNews API with GPT-powered country/language detection and localized search\n",
            "Features: Localized news search, GPT country analysis, Client address risk assessment, Excel export\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "run_enhanced_disaster_analysis()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNcy071tDEgZlnac4zBXqI",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}